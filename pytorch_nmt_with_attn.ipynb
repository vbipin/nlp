{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/vbipin/nlp/blob/master/pytorch_nmt_with_attn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "H1YKD0ecH5YL",
    "outputId": "fcf5deed-9c58-455d-9bff-c6507633237b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 484.0MB 24kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x5cef2000 @  0x7f99cb9b51c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpAQoWoIH9R1"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn6npWWAIlfD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDkn1YIHI45t"
   },
   "outputs": [],
   "source": [
    "#This notebook is adapted from\n",
    "##http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUfcLWs4JJKv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#for monitoring\n",
    "from time import time\n",
    "#for parsing the data filename\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we prepare data directly form the web link. It is useful in Colab notebooks\n",
    "#to convert to script\n",
    "#jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_about.txt', 'fra.txt']\n",
      "154883\n"
     ]
    }
   ],
   "source": [
    "#we need the data from : http://www.manythings.org/anki/fra-eng.zip\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "#get the contents from the website\n",
    "r = requests.get('http://www.manythings.org/anki/fra-eng.zip')\n",
    "\n",
    "#this is one ugly code; But I need the text from a zip file in a url :(((\n",
    "#https://stackoverflow.com/questions/37704836/how-do-i-read-a-csv-file-thats-gzipped-from-url-python\n",
    "#https://codeyarns.com/2013/10/03/how-to-read-contents-of-zip-file-in-python/\n",
    "#https://docs.python.org/2/library/zipfile.html\n",
    "with zipfile.ZipFile( io.BytesIO(r.content), mode='r' ) as zip_file :\n",
    "  print (zip_file.namelist())\n",
    "  lines = zip_file.read('fra.txt').strip().split(b'\\n')\n",
    "  lines = [ str(l, 'utf-8') for l in lines ]\n",
    "  print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have the lines form a file; create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XimE1ug_JPu8"
   },
   "outputs": [],
   "source": [
    "#This class is from the pytorch tutorial. \n",
    "#it holds thevocab and index convertions\n",
    "\n",
    "#XXX TODO: May be use torchtext\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\" :0, \"EOS\" :1}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.word2count = {}\n",
    "\n",
    "    def add_line(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaS5g1e6JWhF"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)     #add a space\n",
    "    s = re.sub(r\"[^a-zA-Z.!?']+\", r\" \", s) #only these; others are spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to929YspJZAP"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#m = re.search( '(...)-(...)\\.txt', 'eng-fra.txt')\n",
    "#m.group(2)\n",
    "class Data :\n",
    "    def __init__(self, filename, src, dest, reverse=False, n_data=-1 ) :\n",
    "        #we do a small hack here to check for file name\n",
    "        if isinstance(filename, list): #must be data coming in\n",
    "          lines = filename\n",
    "        else :\n",
    "          lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "          m = re.search( '(...)-(...)\\.txt', filename)\n",
    "          src, dest = m.group(1), m.group(2)\n",
    "          \n",
    "        lines=lines[0:n_data] #XXXX last pair is not included\n",
    "        \n",
    "                   \n",
    "        # Split every line into pairs and normalize\n",
    "        #self.pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "        \n",
    "        self.pairs = lines\n",
    "        \n",
    "        if reverse : #reverse src and dest\n",
    "            dest, src = src, dest\n",
    "            self.pairs = [ (s[1], s[0]) for s in self.pairs ] #reverse each pair\n",
    "            \n",
    "        self.src = Lang(src) #for each language counts etc\n",
    "        self.dest = Lang(dest)\n",
    "        \n",
    "        for s,d in self.pairs :\n",
    "            self.src.add_line(s)\n",
    "            self.dest.add_line(d)\n",
    "            \n",
    "        #self.seq_len = 1\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        #to tensor\n",
    "        #self.src_lines  = torch.stack( [ self.line_to_tensor(self.src,   s).view(-1,1 ) for s,d in self.pairs ] )\n",
    "        #self.dest_lines = torch.stack( [ self.line_to_tensor(self.dest,  d).view(-1,1 ) for s,d in self.pairs ] )\n",
    "        \n",
    "        #self.tensor_pairs = [torch.Tensor(self.line_to_tensor(self.src,  s).view(-1,1 ), \n",
    "        #                                self.line_to_tensor(self.dest, d).view(-1,1 )) for s,d in self.pairs ] \n",
    "        #self.tensor_pairs = torch.stack( self.tensor_pairs ).to(device)\n",
    "        \n",
    "    def word_to_tensor(self, word, lang=None ) :\n",
    "        if not lang :\n",
    "            lang = self.dest\n",
    "        return torch.LongTensor( [lang.word2index[word]] ).view(-1,1).to(device)\n",
    "    \n",
    "    def index_to_tensor(self, index) :\n",
    "        return torch.LongTensor( [index] ).view(-1,1).to(device)\n",
    "        \n",
    "    def line_to_tensor(self, lang, sentence):\n",
    "        idxs = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "        #idxs.append( lang.EOS_token ) # this is the EOS token\n",
    "        idxs = [lang.SOS_token] + idxs + [lang.EOS_token]\n",
    "        return torch.LongTensor(idxs)\n",
    "            \n",
    "    def batch_(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        N = len(self.pairs)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            st,dt = self.tensor_pairs[i] \n",
    "            yield st, dt\n",
    "            #yield st.to(device), dt.to(device)\n",
    "            \n",
    "    def batch__(self, batch_size=1) : #we return the torchtensor inputs to embedding layers\n",
    "        for s,d in self.pairs : \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n",
    "    \n",
    "    def batch(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        #first we create n_size random indexes for 0 to N\n",
    "        N = len(self.pairs)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            s,d = self.pairs[i] \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_de = open(\"data/multi30k/val.de\", encoding='utf-8').read().strip().split('\\n')\n",
    "lines_en = open(\"data/multi30k/val.en\", encoding='utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [ (normalize_string(lines_de[i]), normalize_string(lines_en[i])) for i in range(len(lines_de)) ]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eine gruppe von mannern ladt baumwolle auf einen lastwagen\n",
      "ein mann schlaft in einem grunen raum auf einem sofa .\n",
      "ein mann mit beginnender glatze der eine rote rettungsweste tragt sitzt in einem kleinen boot .\n",
      "eine frau in einem rotem mantel die eine vermutlich aus asien stammende handtasche in einem blauen farbton halt springt fur einen schnappschuss in die luft .\n",
      "ein madchen sitzt in bequemer haltung an einem offentlichen ort und liest ein buch das sie mit ihrer hand geoffnet halt an der sich ein ring mit einem schmetterling befindet .\n",
      "zwei menschen von denen der eine wie eine nonne gekleidet ist und der andere ein roger smith t shirt tragt rennen bei einem wettlauf in einem bewaldeten gebiet an zuschauern vorbei .\n"
     ]
    }
   ],
   "source": [
    "m, n = 0, 0\n",
    "for i,j in lines :\n",
    "    il = len(i.split(' '))\n",
    "    jl = len(j.split(' '))\n",
    "    if m < il :\n",
    "        print(i)\n",
    "        m = il\n",
    "    if n < jl :\n",
    "        n = jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 29\n"
     ]
    }
   ],
   "source": [
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('viele stuhle sind leer und nur einige wenige menschen genie en die sonne .', 'many of the chairs are empty with only a few people enjoying the sun .')\n"
     ]
    }
   ],
   "source": [
    "#German to English\n",
    "data = Data( lines, 'en', 'de', n_data=1000 )\n",
    "print(random.choice(data.pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iv1mj57Z5wSu",
    "outputId": "a9d0a6bc-7d0b-43e1-b33a-e24b6935dd70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('faisons le .', \"let's do it .\")\n"
     ]
    }
   ],
   "source": [
    "#French to English\n",
    "#data = Data( lines, 'eng', 'fra', reverse=True, n_data=10000 )\n",
    "#print(random.choice(data.pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzl4TVrG9kqS"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, src_vocab_size, hidden_size, num_layers=1 ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_vocab_size, self.enbedding_vector_size )\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #seq_length, batch, enbbding\n",
    "        #print (output.shape)\n",
    "        #print (hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F8gYijaAgbN"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, dest_vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dest_vocab_size, self.enbedding_vector_size )\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, dest_vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        #output = F.relu(output)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #input shape[0] is 1 as wqe feed one input at a time.\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear( output.squeeze() )\n",
    "        #print (output.shape)\n",
    "        output = F.log_softmax( output, dim=0 )\n",
    "        return output.view(1,-1), hidden #output of shape N,C; here N=1\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E_DKBJ5AjkK"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "\n",
    "class Attn(nn.Module) :\n",
    "    def __init__(self, hidden_size, max_length) :\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.linear = nn.Linear(self.hidden_size, self.max_length)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs) :\n",
    "        \n",
    "        attn_scores = self.linear(hidden)\n",
    "        #print(\"attn_scores\", attn_scores.shape )\n",
    "                \n",
    "        attn_weights = F.softmax(attn_scores, dim=2)\n",
    "        \n",
    "        #print(\"attn_weights\", attn_weights.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        \n",
    "        attn_applied = torch.matmul(attn_weights.squeeze(),encoder_outputs)\n",
    "        #print (\"attn_applied \", attn_applied.shape)\n",
    "        \n",
    "        return attn_applied, attn_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.enbedding_vector_size)\n",
    "        \n",
    "        self.attn = Attn(self.hidden_size, self.max_length)\n",
    "        #self.attn = nn.Linear(self.hidden_size, self.max_length)        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"input is an index of the word. We create a word vector out of it\"\"\"\n",
    "        embedded = self.embedding(input) \n",
    "        #print(\"embedded\", embedded.shape )\n",
    "                \n",
    "        \"\"\" gru hidden has shape (num_layers * num_dir, batch, hidden_size)\n",
    "            Here first two dim are 1\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(embedded.view(1,1,-1), hidden)\n",
    "        #print (\"hidden \", hidden.shape)\n",
    "        \n",
    "        #linear W.h \n",
    "        #out (max, )\n",
    "        attn_context, attn_weights = self.attn( hidden, encoder_outputs)\n",
    "        #print (\"attn_context \", attn_context.shape)\n",
    "        \n",
    "        \n",
    "        output = torch.cat((hidden.view(1,-1), attn_context.view(1,-1)), 1)\n",
    "        #print (\"output \", output.shape) \n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        #print (\"output \", output.shape)        \n",
    "        output = F.relu(output) #h tilde\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        #output = F.log_softmax(self.out(output), dim=1)\n",
    "        output = self.out(output)\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SJIEKfqAoVR"
   },
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate( encoder, decoder, data, input_sentence ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = data.line_to_tensor( data.src, input_sentence ).to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    next_word = data.index_to_tensor( data.dest.SOS_token ).to(device)\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        if next_word.item() == data.dest.EOS_token :\n",
    "            break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "        \n",
    "    return \" \".join([ data.dest.index2word[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAefxwrWA8mL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 ) :\n",
    "    start = time()\n",
    "    batch = data.batch(n_data=n_data, random=True)\n",
    "    \n",
    "    loss_db = []\n",
    "    for x, y in batch :\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        h = encoder.initHidden().to(device)\n",
    "        h.detach_()\n",
    "\n",
    "        out, h = encoder(x, h)\n",
    "        g = h\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        for i in range(out.shape[0]) :\n",
    "            encoder_outputs[i] = out[i][0]\n",
    "    \n",
    "        for i in range(len(y) - 1) :\n",
    "        #for i in range(1) :\n",
    "            scores, g, attn_w = decoder( y[i], g, encoder_outputs )\n",
    "            #print(scores.shape)\n",
    "            #print(next_word.shape)\n",
    "            \n",
    "            loss += criterion(scores, y[i+1] )\n",
    "            #next_word = sample_from_scores( scores )  \n",
    "            #next_word = sample_from_softmax( scores )\n",
    "\n",
    "            #next_word = data.index_to_tensor( next_word )\n",
    "\n",
    "        loss.backward()\n",
    "        loss_db.append( float(loss) )\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        if n_data < 0 :\n",
    "            break\n",
    "        else :\n",
    "            n_data -= 1\n",
    "        \n",
    "    end = time()\n",
    "    print (end-start)\n",
    "    return loss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Jl135e7-Ar-U",
    "outputId": "282ffa66-6fb7-4734-99d5-137d2273e0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(2704, 256)\n",
      "  (gru): GRU(256, 256)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(2373, 256)\n",
      "  (attn): Attn(\n",
      "    (linear): Linear(in_features=256, out_features=35, bias=True)\n",
      "  )\n",
      "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (gru): GRU(256, 256)\n",
      "  (out): Linear(in_features=256, out_features=2373, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(data.src.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, data.dest.n_words).to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#criterion = nn.NLLLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "y1QUXIJOA1XO",
    "outputId": "e514b108-6037-4473-c246-f0ec69bd09ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.90017557144165\n"
     ]
    }
   ],
   "source": [
    "avg_loss = []\n",
    "for _ in range(1) :\n",
    "    l = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 )\n",
    "    avg_loss.append( np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Vwrih715BCtO",
    "outputId": "28f1e505-027a-4c0d-9bf5-ece32a3ec9dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f99b05a5d68>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5mf8Uw7-BUv1",
    "outputId": "ecbd0add-e65a-4659-c8e9-071b54dfd0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ein kieferorthopade arbeitet an einem patienten wahrend ein mann die lampe halt .', 'an orthodontist working on a patient while a man holds the light .')\n",
      "SOS a sleeping car truck room unusual is still truck roller dusk is work\n"
     ]
    }
   ],
   "source": [
    "i = 988\n",
    "input_sentence  = data.pairs[i][0]\n",
    "output_sentence = translate( encoder, decoder, data, input_sentence )\n",
    "print(data.pairs[i])\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npzWULkpCHWA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_nmt_with_attn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
