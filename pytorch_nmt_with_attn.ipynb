{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_nmt_with_attn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vbipin/nlp/blob/master/pytorch_nmt_with_attn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "H1YKD0ecH5YL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fcf5deed-9c58-455d-9bff-c6507633237b"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5cef2000 @  0x7f99cb9b51c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QpAQoWoIH9R1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wn6npWWAIlfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDkn1YIHI45t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This notebook is adapted from\n",
        "##http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUfcLWs4JJKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#for monitoring\n",
        "from time import time\n",
        "#for parsing the data filename\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XimE1ug_JPu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\" :0, \"EOS\" :1}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "        self.SOS_token = 0\n",
        "        self.EOS_token = 1\n",
        "        self.word2count = {}\n",
        "\n",
        "    def add_line(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DaS5g1e6JWhF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)     #add a space\n",
        "    s = re.sub(r\"[^a-zA-Z.!?']+\", r\" \", s) #only these; others are spaces\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "to929YspJZAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "#m = re.search( '(...)-(...)\\.txt', 'eng-fra.txt')\n",
        "#m.group(2)\n",
        "class Data :\n",
        "    def __init__(self, filename, src, dest, reverse=False, n_data=-1 ) :\n",
        "        #we do a small hack here to check for file name\n",
        "        if isinstance(filename, list): #must be data coming in\n",
        "          lines = filename\n",
        "        else :\n",
        "          lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "          m = re.search( '(...)-(...)\\.txt', filename)\n",
        "          src, dest = m.group(1), m.group(2)\n",
        "          \n",
        "        lines=lines[0:n_data] #XXXX last pair is not included\n",
        "        \n",
        "                   \n",
        "        # Split every line into pairs and normalize\n",
        "        self.pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "        \n",
        "        if reverse : #reverse src and dest\n",
        "            dest, src = src, dest\n",
        "            self.pairs = [ (s[1], s[0]) for s in self.pairs ] #reverse each pair\n",
        "            \n",
        "        self.src = Lang(src) #for each language counts etc\n",
        "        self.dest = Lang(dest)\n",
        "        \n",
        "        for s,d in self.pairs :\n",
        "            self.src.add_line(s)\n",
        "            self.dest.add_line(d)\n",
        "            \n",
        "        #self.seq_len = 1\n",
        "        self.batch_size = 1\n",
        "        \n",
        "        #to tensor\n",
        "        #self.tensor_pairs = [(self.line_to_tensor(self.src,  s).view(-1,1 ), self.line_to_tensor(self.dest, d).view(-1,1 )) for s,d in self.pairs ] \n",
        "        #self.tensor_pairs = [ (st.to(device),dt.to(device)) for st,dt in self.tensor_pairs ]\n",
        "        \n",
        "    def word_to_tensor(self, word, lang=None ) :\n",
        "        if not lang :\n",
        "            lang = self.dest\n",
        "        return torch.LongTensor( [lang.word2index[word]] ).view(-1,1).to(device)\n",
        "    \n",
        "    def index_to_tensor(self, index) :\n",
        "        return torch.LongTensor( [index] ).view(-1,1).to(device)\n",
        "        \n",
        "    def line_to_tensor(self, lang, sentence):\n",
        "        idxs = [lang.word2index[word] for word in sentence.split(' ')]\n",
        "        #idxs.append( lang.EOS_token ) # this is the EOS token\n",
        "        idxs = [lang.SOS_token] + idxs + [lang.EOS_token]\n",
        "        return torch.LongTensor(idxs)\n",
        "            \n",
        "    def batch_(self, batch_size=1) : #we return the torchtensor inputs to embedding layers\n",
        "        for st,dt in self.tensor_pairs :\n",
        "            yield st.to(device), dt.to(device)\n",
        "            \n",
        "    def batch__(self, batch_size=1) : #we return the torchtensor inputs to embedding layers\n",
        "        for s,d in self.pairs : \n",
        "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
        "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
        "            yield st.to(device), dt.to(device)\n",
        "    \n",
        "    def batch(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
        "        #first we create n_size random indexes for 0 to N\n",
        "        N = len(self.pairs)\n",
        "        r_indexs = np.random.randint(N, size=n_data)\n",
        "        for i in r_indexs :\n",
        "            s,d = self.pairs[i] \n",
        "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
        "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
        "            yield st.to(device), dt.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLTdC9PYJctf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1c16cfe-999e-465a-a462-87a740f49565"
      },
      "cell_type": "code",
      "source": [
        "#we need the data from : http://www.manythings.org/anki/fra-eng.zip\n",
        "import requests\n",
        "import gzip\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "#get the contents from the website\n",
        "r = requests.get('http://www.manythings.org/anki/fra-eng.zip')\n",
        "\n",
        "#this is one ugly code; But I need the text from a zip file in a url :(((\n",
        "#https://stackoverflow.com/questions/37704836/how-do-i-read-a-csv-file-thats-gzipped-from-url-python\n",
        "#https://codeyarns.com/2013/10/03/how-to-read-contents-of-zip-file-in-python/\n",
        "#https://docs.python.org/2/library/zipfile.html\n",
        "with zipfile.ZipFile( io.BytesIO(r.content), mode='r' ) as zip_file :\n",
        "  print (zip_file.namelist())\n",
        "  lines = zip_file.read('fra.txt').strip().split(b'\\n')\n",
        "  lines = [ str(l, 'utf-8') for l in lines ]\n",
        "  print(len(lines))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_about.txt', 'fra.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5L9wqAjBrKhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21f5b3c2-4dfa-45ff-f397-e769aab9c396"
      },
      "cell_type": "code",
      "source": [
        "len(lines)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "metadata": {
        "id": "Iv1mj57Z5wSu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9d0a6bc-7d0b-43e1-b33a-e24b6935dd70"
      },
      "cell_type": "code",
      "source": [
        "#French to English\n",
        "data = Data( lines, 'eng', 'fra', reverse=True, n_data=10000 )\n",
        "print(random.choice(data.pairs))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"j'arrive a le faire .\", 'i can do it .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bzl4TVrG9kqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, src_vocab_size, hidden_size, num_layers=1 ):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        #embedding vector size is fixed as hidden size\n",
        "        self.enbedding_vector_size = hidden_size\n",
        "        self.embedding = nn.Embedding(src_vocab_size, self.enbedding_vector_size )\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(-1)\n",
        "        output = embedded.view( input.shape[0], 1, -1 ) #seq_length, batch, enbbding\n",
        "        #print (output.shape)\n",
        "        #print (hidden.shape)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7F8gYijaAgbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, dest_vocab_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        #embedding vector size is fixed as hidden size\n",
        "        self.enbedding_vector_size = hidden_size\n",
        "        self.embedding = nn.Embedding(dest_vocab_size, self.enbedding_vector_size )\n",
        "        \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, dest_vocab_size)\n",
        "        #self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(-1)\n",
        "        #output = F.relu(output)\n",
        "        output = embedded.view( input.shape[0], 1, -1 ) #input shape[0] is 1 as wqe feed one input at a time.\n",
        "        \n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.linear( output.squeeze() )\n",
        "        #print (output.shape)\n",
        "        output = F.log_softmax( output, dim=0 )\n",
        "        return output.view(1,-1), hidden #output of shape N,C; here N=1\n",
        "\n",
        "    def initHidden(self):\n",
        "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8E_DKBJ5AjkK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 15\n",
        "\n",
        "class Attn(nn.Module) :\n",
        "    def __init__(self, hidden_size, max_length) :\n",
        "        super(Attn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_length = max_length\n",
        "        self.linear = nn.Linear(self.hidden_size, self.max_length)\n",
        "        \n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs) :\n",
        "        \n",
        "        attn_scores = self.linear(hidden)\n",
        "        #print(\"attn_scores\", attn_scores.shape )\n",
        "                \n",
        "        attn_weights = F.softmax(attn_scores, dim=2)\n",
        "        \n",
        "        #print(\"attn_weights\", attn_weights.shape)\n",
        "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
        "        \n",
        "        attn_applied = torch.matmul(attn_weights.squeeze(),encoder_outputs)\n",
        "        #print (\"attn_applied \", attn_applied.shape)\n",
        "        \n",
        "        return attn_applied, attn_weights\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        #embedding vector size is fixed as hidden size\n",
        "        self.enbedding_vector_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.enbedding_vector_size)\n",
        "        \n",
        "        self.attn = Attn(self.hidden_size, self.max_length)\n",
        "        #self.attn = nn.Linear(self.hidden_size, self.max_length)        \n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        #self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        \"\"\"input is an index of the word. We create a word vector out of it\"\"\"\n",
        "        embedded = self.embedding(input) \n",
        "        #print(\"embedded\", embedded.shape )\n",
        "                \n",
        "        \"\"\" gru hidden has shape (num_layers * num_dir, batch, hidden_size)\n",
        "            Here first two dim are 1\n",
        "        \"\"\"\n",
        "        output, hidden = self.gru(embedded.view(1,1,-1), hidden)\n",
        "        #print (\"hidden \", hidden.shape)\n",
        "        \n",
        "        #linear W.h \n",
        "        #out (max, )\n",
        "        attn_context, attn_weights = self.attn( hidden, encoder_outputs)\n",
        "        #print (\"attn_context \", attn_context.shape)\n",
        "        \n",
        "        \n",
        "        output = torch.cat((hidden.view(1,-1), attn_context.view(1,-1)), 1)\n",
        "        #print (\"output \", output.shape) \n",
        "        \n",
        "        output = self.attn_combine(output)\n",
        "        #print (\"output \", output.shape)        \n",
        "        output = F.relu(output) #h tilde\n",
        "        #print (\"output \", output.shape)\n",
        "        \n",
        "        #output = F.log_softmax(self.out(output), dim=1)\n",
        "        output = self.out(output)\n",
        "        #print (\"output \", output.shape)\n",
        "        \n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SJIEKfqAoVR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#debug_list = []\n",
        "def translate( encoder, decoder, data, input_sentence ) :\n",
        "    debug_list = [] #XXX\n",
        "    x = data.line_to_tensor( data.src, input_sentence ).to(device)\n",
        "    h = encoder.initHidden().to(device)\n",
        "    out, h = encoder(x, h)\n",
        "    g = h\n",
        "    \n",
        "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "    for i in range(out.shape[0]) :\n",
        "        encoder_outputs[i] = out[i][0]\n",
        "        \n",
        "    #first input is SOS\n",
        "    next_word = data.index_to_tensor( data.dest.SOS_token ).to(device)\n",
        "    predicted_target = []\n",
        "    for _ in range(25) :        \n",
        "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
        "        #debug_list.append(attn_w)\n",
        "        if next_word.item() == data.dest.EOS_token :\n",
        "            break\n",
        "        predicted_target.append( next_word.item() )\n",
        "        #now we make the next_word from current_word\n",
        "        v, next_word = scores.topk(1) #return value and index\n",
        "        #new_word = data.index_to_tensor( next_word )\n",
        "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
        "        #next_word = torch.multinomial( scores, 1 )[0]\n",
        "        \n",
        "        \n",
        "    return \" \".join([ data.dest.index2word[i] for i in predicted_target ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAefxwrWA8mL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 15\n",
        "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 ) :\n",
        "    start = time()\n",
        "    batch = data.batch(n_data=n_data, random=True)\n",
        "    \n",
        "    loss_db = []\n",
        "    for x, y in batch :\n",
        "        loss = 0\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        \n",
        "        h = encoder.initHidden().to(device)\n",
        "        h.detach_()\n",
        "\n",
        "        out, h = encoder(x, h)\n",
        "        g = h\n",
        "\n",
        "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "        for i in range(out.shape[0]) :\n",
        "            encoder_outputs[i] = out[i][0]\n",
        "    \n",
        "        for i in range(len(y) - 1) :\n",
        "        #for i in range(1) :\n",
        "            scores, g, attn_w = decoder( y[i], g, encoder_outputs )\n",
        "            #print(scores.shape)\n",
        "            #print(next_word.shape)\n",
        "            \n",
        "            loss += criterion(scores, y[i+1] )\n",
        "            #next_word = sample_from_scores( scores )  \n",
        "            #next_word = sample_from_softmax( scores )\n",
        "\n",
        "            #next_word = data.index_to_tensor( next_word )\n",
        "\n",
        "        loss.backward()\n",
        "        loss_db.append( float(loss) )\n",
        "        \n",
        "        decoder_optimizer.step()\n",
        "        encoder_optimizer.step()\n",
        "        if n_data < 0 :\n",
        "            break\n",
        "        else :\n",
        "            n_data -= 1\n",
        "        \n",
        "    end = time()\n",
        "    print (end-start)\n",
        "    return loss_db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jl135e7-Ar-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "282ffa66-6fb7-4734-99d5-137d2273e0ff"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder = EncoderRNN(data.src.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, data.dest.n_words).to(device)\n",
        "\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "#criterion = nn.NLLLoss().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderRNN(\n",
            "  (embedding): Embedding(4068, 256)\n",
            "  (gru): GRU(256, 256)\n",
            ")\n",
            "AttnDecoderRNN(\n",
            "  (embedding): Embedding(2154, 256)\n",
            "  (attn): Attn(\n",
            "    (linear): Linear(in_features=256, out_features=15, bias=True)\n",
            "  )\n",
            "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (gru): GRU(256, 256)\n",
            "  (out): Linear(in_features=256, out_features=2154, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y1QUXIJOA1XO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e514b108-6037-4473-c246-f0ec69bd09ac"
      },
      "cell_type": "code",
      "source": [
        "avg_loss = []\n",
        "for _ in range(20) :\n",
        "    l = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 )\n",
        "    avg_loss.append( np.mean(l))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30.50852394104004\n",
            "30.54999041557312\n",
            "30.741243362426758\n",
            "30.650358200073242\n",
            "30.636716842651367\n",
            "30.51920175552368\n",
            "30.672173500061035\n",
            "30.68585467338562\n",
            "30.536227703094482\n",
            "30.396759033203125\n",
            "30.48255228996277\n",
            "30.49865961074829\n",
            "30.40917992591858\n",
            "30.421916007995605\n",
            "30.693618297576904\n",
            "30.504889726638794\n",
            "30.579080820083618\n",
            "30.525718927383423\n",
            "30.57360553741455\n",
            "30.642094373703003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vwrih715BCtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "28f1e505-027a-4c0d-9bf5-ece32a3ec9dd"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(avg_loss)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d878cf458161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5mf8Uw7-BUv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ecbd0add-e65a-4659-c8e9-071b54dfd0cb"
      },
      "cell_type": "code",
      "source": [
        "i = 9681\n",
        "input_sentence  = data.pairs[i][0]\n",
        "output_sentence = translate( encoder, decoder, data, input_sentence )\n",
        "print(data.pairs[i])\n",
        "print(output_sentence)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"c'est mon compagnon .\", \"he's my partner .\")\n",
            "SOS he's my cat .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "npzWULkpCHWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}