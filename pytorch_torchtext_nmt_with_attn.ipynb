{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/vbipin/nlp/blob/master/pytorch_nmt_with_attn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "H1YKD0ecH5YL",
    "outputId": "fcf5deed-9c58-455d-9bff-c6507633237b"
   },
   "outputs": [],
   "source": [
    "### !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpAQoWoIH9R1"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn6npWWAIlfD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDkn1YIHI45t"
   },
   "outputs": [],
   "source": [
    "#This notebook is adapted from\n",
    "##http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUfcLWs4JJKv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#for monitoring\n",
    "from time import time\n",
    "#for parsing the data filename\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we prepare data directly form the web link. It is useful in Colab notebooks\n",
    "#to convert to script\n",
    "#jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need the data from : http://www.manythings.org/anki/fra-eng.zip\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "#get the contents from the website\n",
    "\"\"\"r = requests.get('http://www.manythings.org/anki/fra-eng.zip')\"\"\"\n",
    "\n",
    "#this is one ugly code; But I need the text from a zip file in a url :(((\n",
    "#https://stackoverflow.com/questions/37704836/how-do-i-read-a-csv-file-thats-gzipped-from-url-python\n",
    "#https://codeyarns.com/2013/10/03/how-to-read-contents-of-zip-file-in-python/\n",
    "#https://docs.python.org/2/library/zipfile.html\n",
    "\"\"\"\n",
    "with zipfile.ZipFile( io.BytesIO(r.content), mode='r' ) as zip_file :\n",
    "  print (zip_file.namelist())\n",
    "  lines = zip_file.read('fra.txt').strip().split(b'\\n')\n",
    "  lines = [ str(l, 'utf-8') for l in lines ]\n",
    "  print(len(lines))\n",
    "\"\"\"\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have the lines form a file; create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XimE1ug_JPu8"
   },
   "outputs": [],
   "source": [
    "#This class is from the pytorch tutorial. \n",
    "#it holds thevocab and index convertions\n",
    "\n",
    "#XXX TODO: May be use torchtext\n",
    "\n",
    "UNK_token = 0\n",
    "PAD_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.stoi = {\"<unk>\":UNK_token, \"<pad>\":PAD_token, \"<sos>\" :SOS_token, \"<eos>\" :EOS_token}\n",
    "        self.itos = {UNK_token:\"<unk>\", PAD_token:\"<pad>\", SOS_token: \"<sos>\", EOS_token: \"<eos>\"}\n",
    "        self.n_words = len(self.stoi)  # Count SOS and EOS\n",
    "        self.SOS_token = SOS_token\n",
    "        self.EOS_token = EOS_token\n",
    "        self.word2count = {}\n",
    "        self.vocab = self.stoi\n",
    "\n",
    "    def add_line(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.stoi:\n",
    "            self.stoi[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.itos[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaS5g1e6JWhF"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)     #add a space\n",
    "    s = re.sub(r\"[^a-zA-Z.!?']+\", r\" \", s) #only these; others are spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to929YspJZAP"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#m = re.search( '(...)-(...)\\.txt', 'eng-fra.txt')\n",
    "#m.group(2)\n",
    "class Data__ :\n",
    "    def __init__(self, src_lines, trg_lines,max_len=35, n_data=None ) :\n",
    "        \n",
    "        if n_data : #we only consider that many lines  \n",
    "            self.src_lines = src_lines[0:n_data]\n",
    "            self.trg_lines = trg_lines[0:n_data]\n",
    "        else :\n",
    "            self.src_lines = src_lines\n",
    "            self.trg_lines = trg_lines\n",
    "            \n",
    "        self.src_lang = Lang('src') #for each language counts etc\n",
    "        self.trg_lang = Lang('trg')\n",
    "        #self.src_lang = src_lang #for each language counts etc\n",
    "        #self.trg_lang = trg_lang\n",
    "        \n",
    "        for s in self.src_lines :\n",
    "            self.src_lang.add_line(s)\n",
    "            \n",
    "        for t in self.trg_lines :\n",
    "            self.trg_lang.add_line(t)\n",
    "            \n",
    "        #self.seq_len = 1\n",
    "        self.batch_size = 1\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        #to tensor\n",
    "        self.src_tensors = torch.stack( [ self.line_to_tensor(self.src_lang, s) for s in self.src_lines ] ).to(device)\n",
    "        self.trg_tensors = torch.stack( [ self.line_to_tensor(self.trg_lang, t) for t in self.trg_lines ] ).to(device)\n",
    "        \n",
    "        \n",
    "    def word_to_tensor(self, word, lang=None ) :\n",
    "        if not lang :\n",
    "            lang = self.trg_lang\n",
    "        return torch.LongTensor( [lang.stoi[word]] ).view(-1,1).to(device)\n",
    "    \n",
    "    def index_to_tensor(self, index) :\n",
    "        return torch.LongTensor( [index] ).view(-1,1).to(device)\n",
    "        \n",
    "    def line_to_tensor(self, lang, sentence):\n",
    "        idxs = [lang.stoi[word] for word in sentence.split(' ')]\n",
    "        #idxs.append( lang.EOS_token ) # this is the EOS token\n",
    "        length = len(idxs)\n",
    "        extend = self.max_len - length - 1 #we take out the lebth and start token and extend that much EOS token\n",
    "        idxs = [lang.SOS_token] + idxs + [lang.EOS_token] * extend\n",
    "        return torch.LongTensor(idxs)\n",
    "            \n",
    "    def train_batch(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        N = len(self.src_lines)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            yield self.src_tensors[i].view(-1,1 ), self.trg_tensors[i].view(-1,1 )\n",
    "            #yield st.to(device), dt.to(device)\n",
    "            \n",
    "    def batch__(self, batch_size=1) : #we return the torchtensor inputs to embedding layers\n",
    "        for s,d in self.pairs : \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n",
    "    \n",
    "    def batch_(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        #first we create n_size random indexes for 0 to N\n",
    "        N = len(self.pairs)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            s,d = self.pairs[i] \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from: https://github.com/pytorch/text/blob/master/test/translation.py\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "    \n",
    "    \n",
    "# Testing custom paths\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en)\n",
    "\n",
    "train, val = datasets.TranslationDataset.splits(\n",
    "    path='.data/multi30k/', train='train',\n",
    "    validation='val', exts=('.de', '.en'),\n",
    "    fields=(DE, EN))\n",
    "\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "print(vars(train[100]))\n",
    "\n",
    "DE.build_vocab(train.src, min_freq=3)\n",
    "EN.build_vocab(train.trg, max_size=50000)\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    (train, val), batch_size=3)\n",
    "\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(len(EN.vocab))\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.src)\n",
    "print(batch.trg)\n",
    "\"\"\"\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data class usnign torchtext\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "class Data :\n",
    "    def __init__(self, batch_size=1) :\n",
    "        DE = data.Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        EN = data.Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        self.train, self.val,self.test = datasets.TranslationDataset.splits(\n",
    "                path='data/multi30k/', train='train',\n",
    "                validation='val', exts=('.de', '.en'),\n",
    "                fields=(DE, EN))\n",
    "        DE.build_vocab(self.train.src,  min_freq=3) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        EN.build_vocab(self.train.trg,  max_size=50000) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        \n",
    "        self.src_lang = DE\n",
    "        self.trg_lang = EN\n",
    "        \n",
    "        self.train_iter, self.val_iter = data.BucketIterator.splits((self.train,self.val), batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def train_batch(self) :\n",
    "        for batch in itertools.islice(self.train_iter,0,len(self.train_iter)) :\n",
    "            yield batch.src, batch.trg\n",
    "        \n",
    "    def val_batch(self) :\n",
    "        for batch in itertools.islice(self.val_iter,0,len(self.val_iter)) :\n",
    "            yield batch.src, batch.trg    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_de = open(\"data/multi30k/val.de\", encoding='utf-8').read().strip().split('\\n')\n",
    "lines_en = open(\"data/multi30k/val.en\", encoding='utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = [ (normalize_string(lines_de[i]), normalize_string(lines_en[i])) for i in range(len(lines_de)) ]\n",
    "#len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German to English\n",
    "#multi30k_data__ = Data__(lines_de, lines_en, max_len=35 )\n",
    "multi30k_data = Data()\n",
    "#print(random.choice(data.pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi30k_data__ = Data__(lines_de, lines_en, max_len=35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.BucketIterator.splits( (multi30k_data.train,multi30k_data.val), repeat=None, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iv1mj57Z5wSu",
    "outputId": "a9d0a6bc-7d0b-43e1-b33a-e24b6935dd70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in itertools.islice(a[0],0,len(a[0])) :\n",
    "    c += 1\n",
    "    if c > 30000 :\n",
    "        break\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzl4TVrG9kqS"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, src_vocab_size, hidden_size, num_layers=1 ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_vocab_size, self.enbedding_vector_size )\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #seq_length, batch, enbbding\n",
    "        #print (output.shape)\n",
    "        #print (hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F8gYijaAgbN"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, dest_vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dest_vocab_size, self.enbedding_vector_size )\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, dest_vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        #output = F.relu(output)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #input shape[0] is 1 as wqe feed one input at a time.\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear( output.squeeze() )\n",
    "        #print (output.shape)\n",
    "        output = F.log_softmax( output, dim=0 )\n",
    "        return output.view(1,-1), hidden #output of shape N,C; here N=1\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E_DKBJ5AjkK"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class Attn(nn.Module) :\n",
    "    def __init__(self, hidden_size, max_length) :\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.linear = nn.Linear(self.hidden_size, self.max_length)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs) :\n",
    "        \n",
    "        attn_scores = self.linear(hidden)\n",
    "        #print(\"attn_scores\", attn_scores.shape )\n",
    "                \n",
    "        attn_weights = F.softmax(attn_scores, dim=2)\n",
    "        \n",
    "        #print(\"attn_weights\", attn_weights.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        \n",
    "        attn_applied = torch.matmul(attn_weights.squeeze(),encoder_outputs)\n",
    "        #print (\"attn_applied \", attn_applied.shape)\n",
    "        \n",
    "        return attn_applied, attn_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.enbedding_vector_size)\n",
    "        \n",
    "        self.attn = Attn(self.hidden_size, self.max_length)\n",
    "        #self.attn = nn.Linear(self.hidden_size, self.max_length)        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"input is an index of the word. We create a word vector out of it\"\"\"\n",
    "        embedded = self.embedding(input) \n",
    "        #print(\"embedded\", embedded.shape )\n",
    "                \n",
    "        \"\"\" gru hidden has shape (num_layers * num_dir, batch, hidden_size)\n",
    "            Here first two dim are 1\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(embedded.view(1,1,-1), hidden)\n",
    "        #print (\"hidden \", hidden.shape)\n",
    "        \n",
    "        #linear W.h \n",
    "        #out (max, )\n",
    "        attn_context, attn_weights = self.attn( hidden, encoder_outputs)\n",
    "        #print (\"attn_context \", attn_context.shape)\n",
    "        \n",
    "        \n",
    "        output = torch.cat((hidden.view(1,-1), attn_context.view(1,-1)), 1)\n",
    "        #print (\"output \", output.shape) \n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        #print (\"output \", output.shape)        \n",
    "        output = F.relu(output) #h tilde\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        #output = F.log_softmax(self.out(output), dim=1)\n",
    "        output = self.out(output)\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SJIEKfqAoVR"
   },
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate( encoder, decoder, in_data, input_sentence ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = in_data.line_to_tensor( in_data.src_lang, input_sentence ).to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    next_word = in_data.index_to_tensor( in_data.trg_lang.SOS_token ).to(device)\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        if next_word.item() == in_data.trg_lang.EOS_token :\n",
    "            break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "        \n",
    "    return \" \".join([ in_data.trg_lang.itos[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAefxwrWA8mL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=5000 ) :\n",
    "    start = time()\n",
    "    #batch = multi30k_data.batch(n_data=n_data, random=True)\n",
    "    train_iter = multi30k_data.train_batch()\n",
    "    \n",
    "    loss_db = []\n",
    "    for x, y in train_iter :\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        h = encoder.initHidden().to(device)\n",
    "        h.detach_()\n",
    "\n",
    "        out, h = encoder(x, h)\n",
    "        g = h\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        for i in range(out.shape[0]) :\n",
    "            encoder_outputs[i] = out[i][0]\n",
    "    \n",
    "        for i in range(len(y) - 1) :\n",
    "        #for i in range(1) :\n",
    "            scores, g, attn_w = decoder( y[i], g, encoder_outputs )\n",
    "            #print(scores.shape)\n",
    "            #print(next_word.shape)\n",
    "            \n",
    "            loss += criterion(scores, y[i+1] )\n",
    "            #next_word = sample_from_scores( scores )  \n",
    "            #next_word = sample_from_softmax( scores )\n",
    "\n",
    "            #next_word = data.index_to_tensor( next_word )\n",
    "\n",
    "        loss.backward()\n",
    "        loss_db.append( float(loss) )\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        if n_data < 0 :\n",
    "            break\n",
    "        else :\n",
    "            n_data -= 1\n",
    "        \n",
    "    end = time()\n",
    "    print (end-start)\n",
    "    return loss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Jl135e7-Ar-U",
    "outputId": "282ffa66-6fb7-4734-99d5-137d2273e0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(5499, 256)\n",
      "  (gru): GRU(256, 256)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(10839, 256)\n",
      "  (attn): Attn(\n",
      "    (linear): Linear(in_features=256, out_features=50, bias=True)\n",
      "  )\n",
      "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (gru): GRU(256, 256)\n",
      "  (out): Linear(in_features=256, out_features=10839, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(len(multi30k_data.src_lang.vocab), hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(multi30k_data.trg_lang.vocab) ).to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#criterion = nn.NLLLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "y1QUXIJOA1XO",
    "outputId": "e514b108-6037-4473-c246-f0ec69bd09ac"
   },
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "for _ in range(20) :\n",
    "    l = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=25000 )\n",
    "    avg_loss.append( np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Vwrih715BCtO",
    "outputId": "28f1e505-027a-4c0d-9bf5-ece32a3ec9dd"
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(decoder.state_dict(), 'decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5mf8Uw7-BUv1",
    "outputId": "ecbd0add-e65a-4659-c8e9-071b54dfd0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> Three women smiling and sitting down . <eos>\n",
      "<sos> Three women are sitting and are walking . <eos>\n",
      "0.14287202148494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bipin/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    }
   ],
   "source": [
    "#i = 188\n",
    "#input_sentence  = multi30k_data.src_lines[i]\n",
    "#input_sentence  = lines_de[i]\n",
    "\n",
    "#inp = \n",
    "\n",
    "#train_iter = multi30k_data.train_batch()\n",
    "inp, out = next(train_iter)\n",
    "target_output   = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in out ]) \n",
    "tout = translate2( encoder, decoder, inp )\n",
    "\n",
    "output_sentence = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in tout ])\n",
    "print(target_output)\n",
    "#print(multi30k_data__.trg_lines[i])\n",
    "print(output_sentence)\n",
    "print( sentence_bleu( [target_output.split(' ')], output_sentence.split(' ') , smoothing_function=SmoothingFunction().method1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npzWULkpCHWA"
   },
   "outputs": [],
   "source": [
    "train_iter = multi30k_data.val_batch()\n",
    "#inp, out = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_val(encoder, decoder, multi30k_data ) :\n",
    "    test_iter = multi30k_data.val_batch()\n",
    "    bleu_score = 0\n",
    "    count = 0\n",
    "    for inp, out in test_iter :\n",
    "        target_output   = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in out ]) \n",
    "        tout = translate2( encoder, decoder, inp )\n",
    "        output_sentence = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in tout ])\n",
    "        bleu_score += sentence_bleu( [target_output.split(' ')], output_sentence.split(' '), \n",
    "                                    smoothing_function=SmoothingFunction().method1)\n",
    "        count += 1\n",
    "    return bleu_score/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-74f0d9812788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti30k_data\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-292-4976f0748c18>\u001b[0m in \u001b[0;36meval_val\u001b[0;34m(encoder, decoder, multi30k_data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtarget_output\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmulti30k_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate2\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmulti30k_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtout\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         bleu_score += sentence_bleu( [target_output.split(' ')], output_sentence.split(' '), \n",
      "\u001b[0;32m<ipython-input-280-9d68c084e82f>\u001b[0m in \u001b[0;36mtranslate2\u001b[0;34m(encoder, decoder, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredicted_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#debug_list.append(attn_w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#if next_word.item() == in_data.trg_lang.EOS_token :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-af0855ddb446>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m#print (\"output \", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m#print (\"output \", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#h tilde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_val(encoder, decoder, multi30k_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = multi30k_data.val_batch(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in test_iter :\n",
    "    c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bipin/anaconda3/envs/pytorch/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/bipin/anaconda3/envs/pytorch/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.609571844608773e-155"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [ str(int(i)) for i in out ]\n",
    "h = [ str(int(i)) for i in tout ]\n",
    "sentence_bleu( [r], h )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46717740367015587"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu( [target_output], output_sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Two',\n",
       " 'women',\n",
       " 'are',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'someone',\n",
       " 'their',\n",
       " 'white',\n",
       " 'top',\n",
       " 'is']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentence.split(' ')[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29126663049139573"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu( [target_output.split(' ')], output_sentence.split(' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "'ensures', 'that', 'the', 'military', 'always',\n",
    "'obeys', 'the', 'commands', 'of', 'the', 'party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
    "'heed', 'Party', 'commands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41180376356915777"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu( [ reference1 ], hypothesis1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate2( encoder, decoder, x ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = x.to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    #next_word = in_data.index_to_tensor( in_data.trg_lang.SOS_token ).to(device)\n",
    "    next_word = x[0]\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        #if next_word.item() == in_data.trg_lang.EOS_token :\n",
    "        #    break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        if next_word.item() == 3 : #in_data.trg_lang.EOS_token :\n",
    "            break\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "    return predicted_target    \n",
    "    #return \" \".join([ in_data.trg_lang.itos[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_nmt_with_attn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
