{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/vbipin/nlp/blob/master/pytorch_nmt_with_attn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "H1YKD0ecH5YL",
    "outputId": "fcf5deed-9c58-455d-9bff-c6507633237b"
   },
   "outputs": [],
   "source": [
    "### !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpAQoWoIH9R1"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn6npWWAIlfD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDkn1YIHI45t"
   },
   "outputs": [],
   "source": [
    "#This notebook is adapted from\n",
    "##http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUfcLWs4JJKv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#for monitoring\n",
    "from time import time\n",
    "#for parsing the data filename\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we prepare data directly form the web link. It is useful in Colab notebooks\n",
    "#to convert to script\n",
    "#jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need the data from : http://www.manythings.org/anki/fra-eng.zip\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "#get the contents from the website\n",
    "\"\"\"r = requests.get('http://www.manythings.org/anki/fra-eng.zip')\"\"\"\n",
    "\n",
    "#this is one ugly code; But I need the text from a zip file in a url :(((\n",
    "#https://stackoverflow.com/questions/37704836/how-do-i-read-a-csv-file-thats-gzipped-from-url-python\n",
    "#https://codeyarns.com/2013/10/03/how-to-read-contents-of-zip-file-in-python/\n",
    "#https://docs.python.org/2/library/zipfile.html\n",
    "\"\"\"\n",
    "with zipfile.ZipFile( io.BytesIO(r.content), mode='r' ) as zip_file :\n",
    "  print (zip_file.namelist())\n",
    "  lines = zip_file.read('fra.txt').strip().split(b'\\n')\n",
    "  lines = [ str(l, 'utf-8') for l in lines ]\n",
    "  print(len(lines))\n",
    "\"\"\"\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have the lines form a file; create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XimE1ug_JPu8"
   },
   "outputs": [],
   "source": [
    "#This class is from the pytorch tutorial. \n",
    "#it holds thevocab and index convertions\n",
    "\n",
    "#XXX TODO: May be use torchtext\n",
    "\n",
    "UNK_token = 0\n",
    "PAD_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.stoi = {\"<unk>\":UNK_token, \"<pad>\":PAD_token, \"<sos>\" :SOS_token, \"<eos>\" :EOS_token}\n",
    "        self.itos = {UNK_token:\"<unk>\", PAD_token:\"<pad>\", SOS_token: \"<sos>\", EOS_token: \"<eos>\"}\n",
    "        self.n_words = len(self.stoi)  # Count SOS and EOS\n",
    "        self.SOS_token = SOS_token\n",
    "        self.EOS_token = EOS_token\n",
    "        self.word2count = {}\n",
    "        self.vocab = self.stoi\n",
    "\n",
    "    def add_line(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.stoi:\n",
    "            self.stoi[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.itos[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaS5g1e6JWhF"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)     #add a space\n",
    "    s = re.sub(r\"[^a-zA-Z.!?']+\", r\" \", s) #only these; others are spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to929YspJZAP"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#m = re.search( '(...)-(...)\\.txt', 'eng-fra.txt')\n",
    "#m.group(2)\n",
    "class Data__ :\n",
    "    def __init__(self, src_lines, trg_lines,max_len=35, n_data=None ) :\n",
    "        \n",
    "        if n_data : #we only consider that many lines  \n",
    "            self.src_lines = src_lines[0:n_data]\n",
    "            self.trg_lines = trg_lines[0:n_data]\n",
    "        else :\n",
    "            self.src_lines = src_lines\n",
    "            self.trg_lines = trg_lines\n",
    "            \n",
    "        self.src_lang = Lang('src') #for each language counts etc\n",
    "        self.trg_lang = Lang('trg')\n",
    "        #self.src_lang = src_lang #for each language counts etc\n",
    "        #self.trg_lang = trg_lang\n",
    "        \n",
    "        for s in self.src_lines :\n",
    "            self.src_lang.add_line(s)\n",
    "            \n",
    "        for t in self.trg_lines :\n",
    "            self.trg_lang.add_line(t)\n",
    "            \n",
    "        #self.seq_len = 1\n",
    "        self.batch_size = 1\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        #to tensor\n",
    "        self.src_tensors = torch.stack( [ self.line_to_tensor(self.src_lang, s) for s in self.src_lines ] ).to(device)\n",
    "        self.trg_tensors = torch.stack( [ self.line_to_tensor(self.trg_lang, t) for t in self.trg_lines ] ).to(device)\n",
    "        \n",
    "        \n",
    "    def word_to_tensor(self, word, lang=None ) :\n",
    "        if not lang :\n",
    "            lang = self.trg_lang\n",
    "        return torch.LongTensor( [lang.stoi[word]] ).view(-1,1).to(device)\n",
    "    \n",
    "    def index_to_tensor(self, index) :\n",
    "        return torch.LongTensor( [index] ).view(-1,1).to(device)\n",
    "        \n",
    "    def line_to_tensor(self, lang, sentence):\n",
    "        idxs = [lang.stoi[word] for word in sentence.split(' ')]\n",
    "        #idxs.append( lang.EOS_token ) # this is the EOS token\n",
    "        length = len(idxs)\n",
    "        extend = self.max_len - length - 1 #we take out the lebth and start token and extend that much EOS token\n",
    "        idxs = [lang.SOS_token] + idxs + [lang.EOS_token] * extend\n",
    "        return torch.LongTensor(idxs)\n",
    "            \n",
    "    def train_batch(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        N = len(self.src_lines)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            yield self.src_tensors[i].view(-1,1 ), self.trg_tensors[i].view(-1,1 )\n",
    "            #yield st.to(device), dt.to(device)\n",
    "            \n",
    "    def batch__(self, batch_size=1) : #we return the torchtensor inputs to embedding layers\n",
    "        for s,d in self.pairs : \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n",
    "    \n",
    "    def batch_(self, n_data=1000, random=True) : #we return the torchtensor inputs to embedding layers\n",
    "        #first we create n_size random indexes for 0 to N\n",
    "        N = len(self.pairs)\n",
    "        r_indexs = np.random.randint(N, size=n_data)\n",
    "        for i in r_indexs :\n",
    "            s,d = self.pairs[i] \n",
    "            st = self.line_to_tensor(self.src,  s).view(-1,1 ) #seq_length, index (n,1)\n",
    "            dt = self.line_to_tensor(self.dest, d).view(-1,1 )#batch need to be handled later\n",
    "            yield st.to(device), dt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from: https://github.com/pytorch/text/blob/master/test/translation.py\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "    \n",
    "    \n",
    "# Testing custom paths\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en)\n",
    "\n",
    "train, val = datasets.TranslationDataset.splits(\n",
    "    path='.data/multi30k/', train='train',\n",
    "    validation='val', exts=('.de', '.en'),\n",
    "    fields=(DE, EN))\n",
    "\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))\n",
    "print(vars(train[100]))\n",
    "\n",
    "DE.build_vocab(train.src, min_freq=3)\n",
    "EN.build_vocab(train.trg, max_size=50000)\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    (train, val), batch_size=3)\n",
    "\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(len(EN.vocab))\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.src)\n",
    "print(batch.trg)\n",
    "\"\"\"\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data class usnign torchtext\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "class Data :\n",
    "    def __init__(self) :\n",
    "        DE = data.Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        EN = data.Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        self.train, self.val,self.test = datasets.TranslationDataset.splits(\n",
    "                path='data/multi30k/', train='train',\n",
    "                validation='val', exts=('.de', '.en'),\n",
    "                fields=(DE, EN))\n",
    "        DE.build_vocab(self.train.src,  min_freq=3) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        EN.build_vocab(self.train.trg,  max_size=50000) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        \n",
    "        self.src_lang = DE\n",
    "        self.trg_lang = EN\n",
    "\n",
    "\n",
    "    def train_batch(self, batch_size=1) :\n",
    "        #train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=batch_size)\n",
    "        #bi = data.BucketIterator.splits((self.val,), batch_size=batch_size)\n",
    "        bi = data.BucketIterator.splits((self.train,), batch_size=batch_size)\n",
    "        train_iter = next(iter(bi))\n",
    "        for batch in train_iter :\n",
    "            yield batch.src, batch.trg\n",
    "        \n",
    "    def val_batch(self, batch_size=1) :\n",
    "        #train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=batch_size)\n",
    "        bi = data.BucketIterator.splits((self.val,), batch_size=batch_size)\n",
    "        val_iter = next(iter(bi))\n",
    "        for batch in val_iter :\n",
    "            yield batch.src, batch.trg    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_de = open(\"data/multi30k/val.de\", encoding='utf-8').read().strip().split('\\n')\n",
    "lines_en = open(\"data/multi30k/val.en\", encoding='utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = [ (normalize_string(lines_de[i]), normalize_string(lines_en[i])) for i in range(len(lines_de)) ]\n",
    "#len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German to English\n",
    "#multi30k_data__ = Data__(lines_de, lines_en, max_len=35 )\n",
    "multi30k_data = Data()\n",
    "#print(random.choice(data.pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k_data__ = Data__(lines_de, lines_en, max_len=35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iv1mj57Z5wSu",
    "outputId": "a9d0a6bc-7d0b-43e1-b33a-e24b6935dd70"
   },
   "outputs": [],
   "source": [
    "#French to English\n",
    "#data = Data( lines, 'eng', 'fra', reverse=True, n_data=10000 )\n",
    "#print(random.choice(data.pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzl4TVrG9kqS"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, src_vocab_size, hidden_size, num_layers=1 ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_vocab_size, self.enbedding_vector_size )\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #seq_length, batch, enbbding\n",
    "        #print (output.shape)\n",
    "        #print (hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F8gYijaAgbN"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, dest_vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dest_vocab_size, self.enbedding_vector_size )\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, dest_vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        #output = F.relu(output)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #input shape[0] is 1 as wqe feed one input at a time.\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear( output.squeeze() )\n",
    "        #print (output.shape)\n",
    "        output = F.log_softmax( output, dim=0 )\n",
    "        return output.view(1,-1), hidden #output of shape N,C; here N=1\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E_DKBJ5AjkK"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "\n",
    "class Attn(nn.Module) :\n",
    "    def __init__(self, hidden_size, max_length) :\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.linear = nn.Linear(self.hidden_size, self.max_length)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs) :\n",
    "        \n",
    "        attn_scores = self.linear(hidden)\n",
    "        #print(\"attn_scores\", attn_scores.shape )\n",
    "                \n",
    "        attn_weights = F.softmax(attn_scores, dim=2)\n",
    "        \n",
    "        #print(\"attn_weights\", attn_weights.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        \n",
    "        attn_applied = torch.matmul(attn_weights.squeeze(),encoder_outputs)\n",
    "        #print (\"attn_applied \", attn_applied.shape)\n",
    "        \n",
    "        return attn_applied, attn_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.enbedding_vector_size)\n",
    "        \n",
    "        self.attn = Attn(self.hidden_size, self.max_length)\n",
    "        #self.attn = nn.Linear(self.hidden_size, self.max_length)        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"input is an index of the word. We create a word vector out of it\"\"\"\n",
    "        embedded = self.embedding(input) \n",
    "        #print(\"embedded\", embedded.shape )\n",
    "                \n",
    "        \"\"\" gru hidden has shape (num_layers * num_dir, batch, hidden_size)\n",
    "            Here first two dim are 1\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(embedded.view(1,1,-1), hidden)\n",
    "        #print (\"hidden \", hidden.shape)\n",
    "        \n",
    "        #linear W.h \n",
    "        #out (max, )\n",
    "        attn_context, attn_weights = self.attn( hidden, encoder_outputs)\n",
    "        #print (\"attn_context \", attn_context.shape)\n",
    "        \n",
    "        \n",
    "        output = torch.cat((hidden.view(1,-1), attn_context.view(1,-1)), 1)\n",
    "        #print (\"output \", output.shape) \n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        #print (\"output \", output.shape)        \n",
    "        output = F.relu(output) #h tilde\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        #output = F.log_softmax(self.out(output), dim=1)\n",
    "        output = self.out(output)\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SJIEKfqAoVR"
   },
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate( encoder, decoder, in_data, input_sentence ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = in_data.line_to_tensor( in_data.src_lang, input_sentence ).to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    next_word = in_data.index_to_tensor( in_data.trg_lang.SOS_token ).to(device)\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        if next_word.item() == in_data.trg_lang.EOS_token :\n",
    "            break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "        \n",
    "    return \" \".join([ in_data.trg_lang.itos[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAefxwrWA8mL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 ) :\n",
    "    start = time()\n",
    "    #batch = multi30k_data.batch(n_data=n_data, random=True)\n",
    "    train_iter = multi30k_data.train_batch()\n",
    "    \n",
    "    loss_db = []\n",
    "    for x, y in train_iter :\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        h = encoder.initHidden().to(device)\n",
    "        h.detach_()\n",
    "\n",
    "        out, h = encoder(x, h)\n",
    "        g = h\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        for i in range(out.shape[0]) :\n",
    "            encoder_outputs[i] = out[i][0]\n",
    "    \n",
    "        for i in range(len(y) - 1) :\n",
    "        #for i in range(1) :\n",
    "            scores, g, attn_w = decoder( y[i], g, encoder_outputs )\n",
    "            #print(scores.shape)\n",
    "            #print(next_word.shape)\n",
    "            \n",
    "            loss += criterion(scores, y[i+1] )\n",
    "            #next_word = sample_from_scores( scores )  \n",
    "            #next_word = sample_from_softmax( scores )\n",
    "\n",
    "            #next_word = data.index_to_tensor( next_word )\n",
    "\n",
    "        loss.backward()\n",
    "        loss_db.append( float(loss) )\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        if n_data < 0 :\n",
    "            break\n",
    "        else :\n",
    "            n_data -= 1\n",
    "        \n",
    "    end = time()\n",
    "    print (end-start)\n",
    "    return loss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Jl135e7-Ar-U",
    "outputId": "282ffa66-6fb7-4734-99d5-137d2273e0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(5499, 256)\n",
      "  (gru): GRU(256, 256)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(10839, 256)\n",
      "  (attn): Attn(\n",
      "    (linear): Linear(in_features=256, out_features=35, bias=True)\n",
      "  )\n",
      "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (gru): GRU(256, 256)\n",
      "  (out): Linear(in_features=256, out_features=10839, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(len(multi30k_data.src_lang.vocab), hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(multi30k_data.trg_lang.vocab) ).to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#criterion = nn.NLLLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "y1QUXIJOA1XO",
    "outputId": "e514b108-6037-4473-c246-f0ec69bd09ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.74745011329651\n",
      "52.19828486442566\n",
      "52.37849283218384\n",
      "52.70231246948242\n",
      "51.98996901512146\n"
     ]
    }
   ],
   "source": [
    "avg_loss = []\n",
    "for _ in range(5) :\n",
    "    l = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 )\n",
    "    avg_loss.append( np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Vwrih715BCtO",
    "outputId": "28f1e505-027a-4c0d-9bf5-ece32a3ec9dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f34cb756198>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VGX6//H3nYRQQoeA9CZVQMDQIVkWQhNBsYG9YqFni7pu8auuZV0DiIoCdsWGoCgICa6bEKQl0psUKaFG6R3k+f2RcX8sG2ACSU5m5vO6rlwzc+Y8mfsx+MnJM2fuY845REQkdIR5XYCIiBQsBb+ISIhR8IuIhBgFv4hIiFHwi4iEGAW/iEiIUfCLiIQYBb+ISIhR8IuIhJgIrwvIScWKFV3t2rW9LkNEJGBkZGT85JyL9mffQhn8tWvXJj093esyREQChplt9ndfLfWIiIQYBb+ISIhR8IuIhBgFv4hIiFHwi4iEGAW/iEiIUfCLiISYoAr+l75Zx9Kt+7wuQ0SkUAua4N935ASTFmzhulfn8syM1Rw98YvXJYmIFEpBE/xlS0SSlBDLza1rMj51I73GpDJvw89elyUiUugETfADlC5WhGf7N2PS/W1xwMAJ8/nT1OUcOHbS69JERAqNoAr+X3WoV5GZw2O5v3MdPlq4he6JqXyzepfXZYmIFApBGfwAxSPDefzqJkx5uCNlihfh3nfSGfbhYn4+dNzr0kREPHXB4Dezhma25IyvA2Y2wszKm1myma3z3ZY7x/g7ffusM7M7834K59eiRlm+HNqJEd3q8/WKHcSPSuWLJdtwzhV0KSIihYLlJgDNLBzYBrQFBgN7nHPPmdmjQDnn3CNn7V8eSAdiAAdkAFc55/ae73ViYmJcfrRlXrvzIH/8bBlLt+6ja6NKPH1dU6qUKZ7nryMiUtDMLMM5F+PPvrld6ukKbHDObQb6Ae/4tr8DXJvD/j2AZOfcHl/YJwM9c/maeabhZaWY8lAH/nx1Y+Zu+InuialMWrCF06d19C8ioSO3wT8A+NB3v7JzbgeA77ZSDvtXA7ae8TjTt+1/mNkgM0s3s/SsrKxcluW/8DDjvs51mTUilqbVyvCnqcu5ZeJ8Nv10ON9eU0SkMPE7+M0sEugLfJqL7285bMvx8No5N945F+Oci4mO9uvqYZekVoUoJt3fluf6N2PltgP0GJ3K+NQNnPrldL6/toiIl3JzxN8L+N459+t5kbvMrAqA73Z3DmMygRpnPK4ObL+YQvODmTGgTU2SE+LoXL8iz8xYw/XjvmPNzgNelyYikm9yE/wD+f/LPADTgF/P0rkT+CKHMbOA7mZWznfWT3fftkLlsjLFmHBHDGMHtiRz71H6vJRGYvIPHD+ltg8iEnz8Cn4zKwHEA1PO2PwcEG9m63zPPefbN8bMJgI45/YATwGLfF9P+rYVOmbGNVdWJTkhjj7Nq/DSN+u4Zmwai7ec9wQkEZGAk6vTOQtKfp3OmRv/WrOLx6euYOeBY9zTsQ6/696AEpERntYkInIu+Xk6Z8j4baPKJI2M5da2NXkj7Ud6jE5l7vqfvC5LROSSKfjPo1SxIjx9bTM+GtSOcDNunbiARz9bxv6javomIoFLwe+HdnUrMHNELA/E1eWT9K3EJ6aQtHKn12WJiFwUBb+fihUJ57Fejfl8cEfKR0Uy6L0Mhkz6np/U9E1EAoyCP5eaVy/LtCGd+F18A5JW7qJbYgpTF2eq6ZuIBAwF/0WIjAhjaNf6TB/WiToVoxj58VLueXsR2/cd9bo0EZELUvBfgvqVSzH5wQ78tU8T5m/cQ3xiCu/N36ymbyJSqCn4L1F4mHFPpzokjYylZc1y/OXzFQwYP5+NWYe8Lk1EJEcK/jxSo3wJ3ru3Df+4vjmrdx6g15g5vJaipm8iUvgo+POQmXFT6xrMTogjrkE0z329hmtfncuq7Wr6JiKFh4I/H1QuXYzXb7+KV29txc79x+j7chovJq1V0zcRKRQU/PnEzOjdrArJI+Po26IqY/+1nqtfSiNjc6HsUSciIUTBn8/KRUWSeFML3r67NUdP/MINr83jiWkrOXz8lNeliUiIUvAXkN80rMSskbHc3q4Wb3+3iR6jU5mzLv8uMSkici4K/gJUsmgET/ZryicPtCcyPIzb31jIHz5dyv4javomIgVHwe+BNnXKM2N4Zx7+TT2mLN5Gt1EpzFyhpm8iUjD8vQJXWTObbGZrzGy1mbU3s4/NbInva5OZLTnH2E1mtty3n7dXVylEihUJ5489G/HF4I5ElyzKg+9n8PAHGew+eMzr0kQkyPl7SakxwEzn3A1mFgmUcM7d/OuTZvYisP8847s453QVkxw0rVaGL4Z0ZHzqRsZ8s46563/mL32acH2rapiZ1+WJSBC64BG/mZUGYoE3AJxzJ5xz+8543oCb+O8LsUsuFAkPY3CXy5kxrDOXVyrJ7z9dyp1vLSJz7xGvSxORIOTPUk9dIAt4y8wWm9lEM4s64/nOwC7n3LpzjHdAkpllmNmgS6w3qF1eqSSfPtCe/+t7Bemb9tB9VCrvfLdJTd9EJE/5E/wRQCtgnHOuJXAYePSM5wdy/qP9js65VkAvYLCZxea0k5kNMrN0M0vPygrd0xzDwow7O9QmaWQsMbXL87dpK7np9XlsUNM3Eckj/gR/JpDpnFvgezyZ7F8EmFkE0B/4+FyDnXPbfbe7galAm3PsN945F+Oci4mOjvZ/BkGqerkSvHN3a/5545Ws232IXmPm8Mq36zmppm8icokuGPzOuZ3AVjNr6NvUFVjlu98NWOOcy8xprJlFmVmpX+8D3YEVl1x1iDAzbriqOskJsXRrXIkXZq2l38tzWbHtfO+ji4icn7/n8Q8FPjCzZUAL4Bnf9gGctcxjZlXNbIbvYWUgzcyWAguB6c65mZdedmipVKoYr956Fa/d1ordB4/T75W5PD9zDcdOqumbiOSeFcZrxcbExLj0dJ3yn5P9R07y9PRVfJqRSd2KUTx/Q3Na1y7vdVki4jEzy3DOxfizrz65G2DKlCjCCzdeybv3tOH4qdPc+No8/vrFCg6p6ZuI+EnBH6BiG0STNDKWuzrU5r35m+kxKpWUH0L3bCgR8Z+CP4BFFY3gib5XMPnB9hQrEsadby4k4ZMl7DtywuvSRKQQU/AHgatqlWf6sM4M6XI505Zsp1tiCjOW76Awvn8jIt5T8AeJYkXC+X2PhnwxpCOXlSnGwx98z4PvZ7D7gJq+ich/U/AHmSuqluHzhzvySM9GfLs2i26JKXySvlVH/yLyHwr+IBQRHsZDv6nHzOGdaXRZaf44eRm3v7GQrXvU9E1EFPxBrW50ST4a1I6nrm3K4i176T4qlbfm/sgvavomEtIU/EEuLMy4vV0tkhLiaFu3PP/35SpufO071u8+6HVpIuIRBX+IqFa2OG/d1ZpRN1/Jxp8O03tMGmO/WaembyIhSMEfQsyM61pWZ3ZCHPFXVObF5B+4ZmwayzPV9E0klCj4Q1DFkkV55ZZWvH77Vew5fIJ+r6Tx7Ner1fRNJEQo+ENYjysuIzkhjptiavB6ykZ6jZnDgo0/e12WiOQzBX+IK1O8CM9d35wP7mvLqdOnuXn8fP78+XIOHjvpdWkikk8U/AJAx8srMmtELPd2qsMHC7bQY1Qq367Z7XVZIpIPFPzyHyUiI/hLnyZ89lAHoopGcPfbixj58RL2HFbTN5FgouCX/9GqZjm+GtaJYV3r8+XS7cQnpvDl0u1q+yASJPwKfjMra2aTzWyNma02s/Zm9oSZbTOzJb6v3ucY29PM1prZejN7NG/Ll/xSNCKchPgGfDm0E9XKFWfoh4u5/90Mdqnpm0jA8/eIfwww0znXCLgSWO3bPso518L3NePsQWYWDrwC9AKaAAPNrEke1C0FpHGV0kx5qAN/6t2IOeuym759tHCLjv5FAtgFg9/MSgOxwBsAzrkTzrl9fn7/NsB659xG59wJ4COg38UWK96ICA9jUGw9Zo2IpUmV0jw6ZTm3TlzAlp/V9E0kEPlzxF8XyALeMrPFZjbRzKJ8zw0xs2Vm9qaZlcthbDVg6xmPM33bJADVrhjFh/e345nrmrEscz/dR6cwcc5GNX0TCTD+BH8E0AoY55xrCRwGHgXGAfWAFsAO4MUcxloO23JMCTMbZGbpZpaelaVrxxZWYWHGLW1rkpwQS4d6FXl6+mr6j/uOtTvV9E0kUPgT/JlApnNuge/xZKCVc26Xc+4X59xpYALZyzo5ja1xxuPqwPacXsQ5N945F+Oci4mOjvZ/BuKJKmWK88adMYwZ0IKte47QZ+wcRs/+gROn1PRNpLC7YPA753YCW82soW9TV2CVmVU5Y7frgBU5DF8E1DezOmYWCQwApl1izVJImBn9WlQjeWQsvZtVYfTsdVwzNo2lW/19C0hEvODvWT1DgQ/MbBnZSzvPAP8ws+W+bV2AkQBmVtXMZgA4504BQ4BZZJ8J9IlzbmUez0E8VqFkUcYMaMnEO2LYf/Qk1706l79PX8XRE2r6JlIYWWE8LS8mJsalp6d7XYZchAPHTvLc12uYtGALtSqU4Ln+zWlfr4LXZYkEPTPLcM7F+LOvPrkreap0sSI8c10zJt3fFoCBE+bz2JTlHFDTN5FCQ8Ev+aJDvYrMHB7LoNi6fLxoC/GJKcxetcvrskQEBb/ko+KR4fypd2OmPNyRssUjue/ddIZ9uJifDx33ujSRkKbgl3zXokZZvhzaiZHdGvD1ih10S0zhiyXb1PZBxCMKfikQkRFhDO9Wn+nDOlOrQhTDP1rCfe+ks2P/Ua9LEwk5Cn4pUA0ql+Kzhzrw56sbM3fDT8QnpvLBgs2cVtsHkQKj4JcCFx5m3Ne5Lkkj4mhevQyPT13BLRPns+mnw16XJhISFPzimZoVSvDBfW15rn8zVm47QI/RqYxP3cCpX9T2QSQ/KfjFU2bGgDY1SU6Io3P9aJ6ZsYb+475j9Y4DXpcmErQU/FIoXFamGBPuuIqXb2nJtr1HuWZsGonJP3D8lNo+iOQ1Bb8UGmZGn+ZVmZ0QxzVXVuWlb9bR56U0vt+y1+vSRIKKgl8KnXJRkYy6uQVv3dWaQ8dPcf2473jqq1UcOXHK69JEgoKCXwqtLo0qkTQyllvb1uSNtB/pMTqVuet/8roskYCn4JdCrVSxIjx9bTM+HtSOiLAwbp24gEcmL2P/UTV9E7lYCn4JCG3rVuDr4Z15MK4ek7/PJD4xhaSVO70uSyQgKfglYBQrEs6jvRrx+cMdqVCyKIPey2DwpO/JOqimbyK5oeCXgNOsehmmDenI77s3IHnlLuJHpTB1caaavon4ya/gN7OyZjbZzNaY2Woza29mL/geLzOzqWZW9hxjN/ku0bjEzHRZLckTRcLDGPLb+swY3om6FaMY+fFS7n57Edv2qembyIX4e8Q/BpjpnGsEXEn29XOTgabOuebAD8Bj5xnfxTnXwt/Lgon46/JKpfj0wQ787ZomLNi4h+6JKbw3b5OavomcxwWD38xKA7HAGwDOuRPOuX3OuSTfxdQB5gPV869MkXMLDzPu7liHpJGxtKpVjr98sZIB4+ezMeuQ16WJFEr+HPHXBbKAt8xssZlNNLOos/a5B/j6HOMdkGRmGWY26BJqFTmvGuVL8O49bXjhhuas2XmAnmPmMO7favomcjZ/gj8CaAWMc861BA4Dj/76pJk9DpwCPjjH+I7OuVZAL2CwmcXmtJOZDTKzdDNLz8rKys0cRP7DzLgxpgazE+Lo0jCa52eu4dpX57Jqu5q+ifzKn+DPBDKdcwt8jyeT/YsAM7sT6APc6s5xSoVzbrvvdjcwFWhzjv3GO+dinHMx0dHRuZuFyFkqlS7G67fHMO7WVuzcf5y+L6fxz1lrOXZSTd9ELhj8zrmdwFYza+jb1BVYZWY9gUeAvs65IzmNNbMoMyv1632gO7AiTyoX8UOvZlWYnRBLvxbVePnb9Vz90hwyNu/xuiwRT/l7Vs9Q4AMzWwa0AJ4BXgZKAcm+UzVfAzCzqmY2wzeuMpBmZkuBhcB059zMPJ2ByAWULRHJizddyTv3tOHYydPc8No8npi2ksPH1fRNQpMVxg+9xMTEuPR0nfIvee/Q8VO8MHMN787fTNUyxXm2fzNiG2hpUQKfmWX4e8q8PrkrIaVk0Qj+r19TPnmgPUWLhHHHmwv5/adL2X9ETd8kdCj4JSS1rl2eGcM68/Bv6jF18Ta6jUph5oodXpclUiAU/BKyihUJ5489G/HF4I5ElyzKg+9/z0PvZ7D74DGvSxPJVwp+CXlNq5XhiyEd+UOPhnyzZjfxialMzlDTNwleCn4Rspu+De5yOTOGdaZ+pZL8/tOl3PHmQrbuyfFMZZGApuAXOcPllUryyQPtebLfFXy/eS89Rqfy9twf1fRNgoqCX+QsYWHGHe1rM2tkLDG1y/PEl6u46fV5rN+tpm8SHBT8IudQvVwJ3rm7NS/eeCXrdh+i95g5vPLtek6q6ZsEOAW/yHmYGddfVZ3ZCXF0a1KJF2atpd/Lc1mxbb/XpYlcNAW/iB+iSxXl1Vuv4rXbWpF16Dj9XpnL8zPXqOmbBCQFv0gu9Gxahdkj47i+VTXG/XsDvcfMYdEmNX2TwKLgF8mlMiWK8I8bruT9e9ty4pfT3PjaPP76xQoOqembBAgFv8hF6lS/IrNGxHJ3x9q8N38zPUal8u+1u70uS+SCFPwilyCqaAR/u+YKJj/YgeKR4dz11iISPlnC3sMnvC5N5JwU/CJ54Kpa5Zg+rBNDf3s505ZsJ35UCtOX7VDbBymUFPwieaRoRDi/696QaUM6UaVMcQZP+p4H3stg9wE1fZPCRcEvkseaVC3N1Ic78FivRqT8kEXXxBQ+WbRVR/9SaPgV/GZW1swmm9kaM1ttZu3NrLyZJZvZOt9tuXOMvdO3zzrfxdlFgl5EeBgPxNXj6+GdaVylNH/8bBm3v6Gmb1I4+HvEPwaY6ZxrBFwJrAYeBb5xztUHvvE9/i9mVh74G9AWaAP87Vy/IESCUd3oknx0fzuevrYpS7buo/uoVN5M+5Ff1PRNPHTB4Dez0kAs8AaAc+6Ec24f0A94x7fbO8C1OQzvASQ75/Y45/YCyUDPvChcJFCEhRm3tatF0shY2tYtz5NfreLG175j3a6DXpcmIcqfI/66QBbwlpktNrOJZhYFVHbO7QDw3VbKYWw1YOsZjzN92/6HmQ0ys3QzS8/KysrVJEQCQdWyxXnrrtaMvrkFP/50mKtfSmPsN+s4cUpN36Rg+RP8EUArYJxzriVwmByWdc7BctiW49+4zrnxzrkY51xMdHS0n99eJLCYGde2rEZyQhw9ml7Gi8k/0PflNJZl7vO6NAkh/gR/JpDpnFvgezyZ7F8Eu8ysCoDvNqePLGYCNc54XB3YfvHligSHiiWLMnZgSybcEcPeIye49pW5PDtjtZq+SYG4YPA753YCW82soW9TV2AVMA349SydO4Evchg+C+huZuV8b+p2920TESC+SWWSRsZxc+savJ66kZ6jU5m/8Wevy5Ig5+9ZPUOBD8xsGdACeAZ4Dog3s3VAvO8xZhZjZhMBnHN7gKeARb6vJ33bRMSnTPEiPNu/OZPua8tpBwPGz+fxqcs5eOyk16VJkLLC+KGSmJgYl56e7nUZIgXuyIlTJCb9wJtzf6Ry6WI8c10zujTK6bwJkf9mZhnOuRh/9tUnd0UKkRKREfy5TxM+e6gDJYtGcPfbixjx0WL2qOmb5CEFv0gh1LJmOb4a1onhXeszffkOuiWmMG3pdrV9kDyh4BcppIpGhDMyvgFfDu1EjXLFGfbhYu5/N4Od+9X0TS6Ngl+kkGt0WWmmPNyRx3s3Jm19FvGJKXy4cIuO/uWiKfhFAkB4mHF/bF1mDo/limqleWzKcm6ZsIDNPx/2ujQJQAp+kQBSu2IUk+5rxzPXNWPFtv30GJ3KxDkb1fRNckXBLxJgwsKMW9rWJCkhlo71KvL09NX0H/cda3eq6Zv4R8EvEqCqlCnOxDtjeGlgS7buOUKfsXMYPfsHNX2TC1LwiwQwM6PvlVWZnRBH72ZVGD17HdeMTWPJVjV9k3NT8IsEgfJRkYwZ0JI37oxh/9GT9H91Ln+fvoqjJ9T0Tf6Xgl8kiHRtXJmkhFgGtKnJhDk/0mN0Kt9t+MnrsqSQUfCLBJnSxYrwzHXN+PD+dpjBLRMW8NiU5RxQ0zfxUfCLBKn29Sowc3gsD8TW5eNFW4hPTGH2ql1elyWFgIJfJIgVjwznsd6N+XxwR8qViOS+d9MZ+uFifj503OvSxEMKfpEQ0Lx6WaYN6URCfANmrshu+vbFkm1q+xCiFPwiISIyIoxhXeszfVhnalWIYvhHS7j3nXS27zvqdWlSwPy6EIuZbQIOAr8Ap5xzMWb2MfDr5RjLAvuccy38GXuh19OFWETy1y+nHW9/t4l/zlpLeJjxWO9GDGxdk7Aw87o0uUi5uRBLRC6+bxfn3H/OC3PO3XzGC74I7Pd3rIh4KzzMuLdTHeIbV+axqct4fOoKpi3ZznPXN6dOxSivy5N8dslLPWZmwE3Ah5dejogUpJoVSvD+vW15/vpmrNpxgJ6jU3k9ZQOnflHbh2Dmb/A7IMnMMsxs0FnPdQZ2OefWXcRYEfGYmXFz65rMTogjtkE0z369hv7jvmP1jgNelyb5xN/g7+icawX0AgabWewZzw3k/Ef75xv7H2Y2yMzSzSw9KyvLz7JEJK9ULl2M8bdfxSu3tGL7vqNcMzaNxKS1HD+ltg/Bxq83d/9rgNkTwCHn3D/NLALYBlzlnMvMzdjz7ac3d0W8tffwCZ76ahVTFm+jfqWSPH9Dc1rVLOd1WXIeuXlz94JH/GYWZWalfr0PdAdW+J7uBqw5V+hfYKyIFFLloiJJvLkFb93dmsPHT3H9uO948stVHDlxyuvSJA/4s9RTGUgzs6XAQmC6c26m77kBnLXMY2ZVzWyGH2NFpJDr0rASs0bGclvbWrw5N7vpW9o6naAX6HK91FMQtNQjUvgs/HEPj3y2jB9/OsxNMdV5/OomlClexOuyxCdPl3pERADa1CnP18M789Bv6vHZ99uIT0xh1sqdXpclF0HBLyJ+K1YknEd6NuLzhztSoWRRHngvg8EffE/WQTV9CyQKfhHJtWbVyzBtSEf+0KMhyat2ET8qhSnfZ6rpW4BQ8IvIRSkSHsbgLpczY3gn6laMIuGTpdz11iK2qelboafgF5FLcnmlUnz6YAeeuKYJizbtoXtiCu/O28Tp0zr6L6wU/CJyycLDjLs61mHWiFha1SrHX79Yyc3j57Eh65DXpUkOFPwikmdqlC/Bu/e04YUbmrN250F6jZnDq/9er6ZvhYyCX0TylJlxY0wNZv8ujt82rMQ/Zq7l2lfnsnL7+Tq3S0FS8ItIvqhUqhiv3X4V425txc79x+n78lxemLWGYyfV9M1rCn4RyVe9mlVhdkIs17WsxivfbuDql+aQvmmP12WFNAW/iOS7siUi+eeNV/LuPW04dvI0N74+jyemreTwcTV984KCX0QKTGyDaJJGxnJn+9q8M28T3UelkvqDrr9R0BT8IlKgoopG8ETfK/j0gfYULRLGHW8u5PefLmXfkRNelxYyFPwi4omY2uWZMawzg7vUY+ribXRLTOXr5Tu8LiskKPhFxDPFioTzhx6NmDakI5VLF+WhD77nofcz2H3wmNelBTUFv4h47oqqZfh8cEce6dmIb9bsJj4xlU/Tt6rpWz5R8ItIoVAkPIyHflOPr4d3pkHlkvxh8jLueHMhW/cc8bq0oONX8JvZJjNbbmZLzCzdt+0JM9vm27bEzHqfY2xPM1trZuvN7NG8LF5Egk+96JJ8PKg9T/W7gu8376XH6FTenvujmr7lodwc8XdxzrU469Jeo3zbWjjnZpw9wMzCgVeAXkATYKCZNbm0kkUk2IWFGbe3r82skbG0rl2eJ75cxY2vz2P97oNelxYU8nuppw2w3jm30Tl3AvgI6JfPrykiQaJ6uRK8fXdrEm+6kg1Zh+g9Jo1Xvl3PSTV9uyT+Br8Dkswsw8wGnbF9iJktM7M3zaxcDuOqAVvPeJzp2yYi4hczo3+r6iSPjCP+isq8MGst/V6ey4ptavp2sfwN/o7OuVZkL9kMNrNYYBxQD2gB7ABezGGc5bAtx4U6MxtkZulmlp6VpU/yich/iy5VlFduacXrt19F1qHj9HtlLs/PVNO3i+FX8DvntvtudwNTgTbOuV3OuV+cc6eBCWQv65wtE6hxxuPqwPZzvMZ451yMcy4mOjo6N3MQkRDS44rLmD0yjhtaVWfcvzfQe8wcFv6opm+5ccHgN7MoMyv1632gO7DCzKqcsdt1wIochi8C6ptZHTOLBAYA0y69bBEJZWVKFOH5G5rz/r1tOfHLaW56fR5/+XwFh9T0zS/+HPFXBtLMbCmwEJjunJsJ/MN3iucyoAswEsDMqprZDADn3ClgCDALWA184pxbmQ/zEJEQ1Kl+RZJGxnJPxzq8v2Az3RNT+Hbtbq/LKvSsMH4yLiYmxqWnp3tdhogEkIzNe3nks2Ws332I/i2r8Zc+TSgXFel1WQXGzDLOOt3+nPTJXREJClfVKsf0YZ0Y9tvLmbZ0O/GjUpi+bIfaPuRAwS8iQaNoRDgJ3Rvy5dBOVClTnMGTvueB9zLYdUBN386k4BeRoNO4SmmmPtyBx3o1IuWHLLolpvDxoi06+vdR8ItIUIoID+OBuHrMHBFL4yqleeSz5dz2xgK2/Kymbwp+EQlqdSpG8dH97Xj62qYs3bqfHqNTeSPtR34J4aZvCn4RCXphYcZt7WqRNDKWdnXL89RXq7jhte9Ytys0m74p+EUkZFQtW5w372rNmAEt2PTTYa5+KY2XvlnHiVOh1fRNwS8iIcXM6NeiGrMT4ujR9DISk3+g78tpLN26z+vSCoyCX0RCUoWSRRk7sCUT7ohh75ETXPfqXJ6dsZqjJ4K/6ZuCX0RCWnyTyiQnxHFz6xq8nrqRXmNSmb/xZ6/LylcKfhEJeaWLFeHZ/s2ZdF9bTjsYMH4+j09dzsFjJ70uLV8o+EVEfDpcXpFZI2K5v3PCpgz6AAAHmElEQVQdPly4he6jUvnXml1el5XnFPwiImcoHhnO41c3YcrDHSldrAj3vJ3O8I8W8/Oh416XlmcU/CIiOWhRoyxfDu3EiG71mbF8B/GjUpm2dHtQtH1Q8IuInENkRBgjujXgq6GdqVG+BMM+XMz976azc39gN31T8IuIXEDDy0ox5aEO/PnqxqSt/4n4xBQ+XBi4Td8U/CIifggPM+7rXJdZI2JpWq0Mj01Zzi0TFrD558Nel5ZrfgW/mW3yXWZxiZml+7a9YGZrzGyZmU01s7L+jhURCVS1KkQx6f62PNu/GSu2ZTd9m5C6MaCavuXmiL+Lc67FGZf2SgaaOueaAz8Aj+VirIhIwDIzBrapSXJCHJ0ur8jfZ6ym/6tzWbszMJq+XfRSj3MuyXcxdYD5QPW8KUlEJDBcVqYYE+6IYezAlmTuPUqfsXMYlfxDoW/65m/wOyDJzDLMbFAOz98DfH2RYwEws0Fmlm5m6VlZWX6WJSLiLTPjmiurkpwQx9XNqjDmm3X0GTuHJYW46Zv58660mVV1zm03s0pkL/EMdc6l+p57HIgB+rscvtn5xp5LTEyMS0/X2wEiEnj+tWYXj09dwa4Dx7inYx1+170hxSPD8/11zSzD3+V0v474nXPbfbe7galAG98L3Qn0AW7NKfTPN1ZEJBj9tlFlkkbGMrBNTSam/UiP0al8t+Enr8v6LxcMfjOLMrNSv94HugMrzKwn8AjQ1zmX40UszzU2r4oXESmMShUrwt+va8ZHg9oRZnDLhAU8NmUZ+48WjqZv/hzxVwbSzGwpsBCY7pybCbwMlAKSfadqvgbZSztmNuMCY0VEgl67uhWYOSKWB+Lq8vGirXQflULyKu+bvvm1xl/QtMYvIsFmWeY+/jh5GWt2HqRP8yo80fcKKpYsmmffP8/X+EVE5NI0r16WaUM68bv4BiSt3EV8YgqfL97mSdsHBb+ISAGJjAhjaNf6TB/WidoVoxjx8RLufSed7fuOFmgdCn4RkQJWv3IpJj/Ygb/2acK8DT/TfVQq78/fzOkCavug4BcR8UB4mHFPpzokjYylRY2y/PnzFQyYMJ8jJ05dePAlisj3VxARkXOqUb4E793bhk/TM8nYvJcSkfkfywp+ERGPmRk3ta7BTa1rFMjraalHRCTEKPhFREKMgl9EJMQo+EVEQoyCX0QkxCj4RURCjIJfRCTEKPhFREJMoWzLbGZZwOaLHF4RKFyXu8l/mnPwC7X5guacW7Wcc9H+7Fgog/9SmFm6vz2pg4XmHPxCbb6gOecnLfWIiIQYBb+ISIgJxuAf73UBHtCcg1+ozRc053wTdGv8IiJyfsF4xC8iIucRsMFvZj3NbK2ZrTezR3N4vqiZfex7foGZ1S74KvOOH/NNMLNVZrbMzL4xs1pe1JmXLjTnM/a7wcycmQX8GSD+zNnMbvL9rFea2aSCrjGv+fFvu6aZfWtmi33/vnt7UWdeMbM3zWy3ma04x/NmZi/5/nssM7NWeV6Ecy7gvoBwYANQF4gElgJNztrnYeA13/0BwMde153P8+0ClPDdfyiQ5+vvnH37lQJSgflAjNd1F8DPuT6wGCjne1zJ67oLYM7jgYd895sAm7yu+xLnHAu0Alac4/newNeAAe2ABXldQ6Ae8bcB1jvnNjrnTgAfAf3O2qcf8I7v/mSgq5lZAdaYly44X+fct865I76H84HqBVxjXvPnZwzwFPAP4FhBFpdP/Jnz/cArzrm9AM653QVcY17zZ84OKO27XwbYXoD15TnnXCqw5zy79APeddnmA2XNrEpe1hCowV8N2HrG40zfthz3cc6dAvYDFQqkurznz3zPdC/ZRwyB7IJzNrOWQA3n3FcFWVg+8ufn3ABoYGZzzWy+mfUssOryhz9zfgK4zcwygRnA0IIpzTO5/f891wL1mrs5HbmffXqSP/sECr/nYma3ATFAXL5WlP/OO2czCwNGAXcVVEEFwJ+fcwTZyz2/Ifuvujlm1tQ5ty+fa8sv/sx5IPC2c+5FM2sPvOeb8+n8L88T+Z5dgXrEnwmceVXi6vzvn3//2cfMIsj+E/F8f14VZv7MFzPrBjwO9HXOHS+g2vLLheZcCmgK/NvMNpG9FjotwN/g9fff9RfOuZPOuR+BtWT/IghU/sz5XuATAOfcPKAY2T1tgpVf/79fikAN/kVAfTOrY2aRZL95O+2sfaYBd/ru3wD8y/neOQlAF5yvb9njdbJDP9DXfeECc3bO7XfOVXTO1XbO1Sb7fY2+zrl0b8rNE/78u/6c7DfyMbOKZC/9bCzQKvOWP3PeAnQFMLPGZAd/VoFWWbCmAXf4zu5pB+x3zu3IyxcIyKUe59wpMxsCzCL7rIA3nXMrzexJIN05Nw14g+w/CdeTfaQ/wLuKL42f830BKAl86nsPe4tzrq9nRV8iP+ccVPyc8yygu5mtAn4B/uCc+9m7qi+Nn3P+HTDBzEaSveRxVwAfxGFmH5K9VFfR977F34AiAM6518h+H6M3sB44Atyd5zUE8H8/ERG5CIG61CMiIhdJwS8iEmIU/CIiIUbBLyISYhT8IiIhRsEvIhJiFPwiIiFGwS8iEmL+H1AdfcWdH5+rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5mf8Uw7-BUv1",
    "outputId": "ecbd0add-e65a-4659-c8e9-071b54dfd0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> A woman singing into a microphone while a man plays drums in the background . <eos>\n",
      "<sos> A woman in a black shirt is standing on a city street . <eos> <eos> <eos> . <eos> <eos> <eos> . <eos> <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "i = 188\n",
    "#input_sentence  = multi30k_data.src_lines[i]\n",
    "#input_sentence  = lines_de[i]\n",
    "\n",
    "#inp = \n",
    "\n",
    "#train_iter = multi30k_data.train_batch()\n",
    "inp, out = next(train_iter)\n",
    "target_output   = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in out ]) \n",
    "tout = translate2( encoder, decoder, inp )\n",
    "\n",
    "output_sentence = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in tout ])\n",
    "print(target_output)\n",
    "#print(multi30k_data__.trg_lines[i])\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> A man dressed in <unk> holding the puck . <eos> <eos> <eos> <eos> <eos> a red with her party . <eos> <eos> <eos> <eos>'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in output_sentence ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npzWULkpCHWA"
   },
   "outputs": [],
   "source": [
    "train_iter = multi30k_data.train_batch()\n",
    "inp, out = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-fdfd1de89a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti30k_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device, train)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 10)"
     ]
    }
   ],
   "source": [
    "inp = multi30k_data.src_lang.numericalize( input_sentence.split(\" \") )\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> A man wearing olive drab clothing is holding himself up off the ground with his hands . <eos>'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in out ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate2( encoder, decoder, x ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = x.to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    #next_word = in_data.index_to_tensor( in_data.trg_lang.SOS_token ).to(device)\n",
    "    next_word = x[0]\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        #if next_word.item() == in_data.trg_lang.EOS_token :\n",
    "        #    break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "    return predicted_target    \n",
    "    #return \" \".join([ in_data.trg_lang.itos[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_nmt_with_attn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
