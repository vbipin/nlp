{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/vbipin/nlp/blob/master/pytorch_nmt_with_attn.ipynb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "H1YKD0ecH5YL",
    "outputId": "fcf5deed-9c58-455d-9bff-c6507633237b"
   },
   "source": [
    "# !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpAQoWoIH9R1"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn6npWWAIlfD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDkn1YIHI45t"
   },
   "outputs": [],
   "source": [
    "#This notebook is adapted from\n",
    "##http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUfcLWs4JJKv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#for monitoring\n",
    "from time import time\n",
    "#for parsing the data filename\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we prepare data directly form the web link. It is useful in Colab notebooks\n",
    "#to convert to script\n",
    "#jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need the data from : http://www.manythings.org/anki/fra-eng.zip\n",
    "#import requests\n",
    "#import gzip\n",
    "#import io\n",
    "#import zipfile\n",
    "\n",
    "#get the contents from the website\n",
    "#r = requests.get('http://www.manythings.org/anki/fra-eng.zip')\n",
    "\n",
    "#this is one ugly code; But I need the text from a zip file in a url :(((\n",
    "#https://stackoverflow.com/questions/37704836/how-do-i-read-a-csv-file-thats-gzipped-from-url-python\n",
    "#https://codeyarns.com/2013/10/03/how-to-read-contents-of-zip-file-in-python/\n",
    "#https://docs.python.org/2/library/zipfile.html\n",
    "\n",
    "\n",
    "#with zipfile.ZipFile( io.BytesIO(r.content), mode='r' ) as zip_file :\n",
    "#  print (zip_file.namelist())\n",
    "#  lines = zip_file.read('fra.txt').strip().split(b'\\n')\n",
    "#  lines = [ str(l, 'utf-8') for l in lines ]\n",
    "#  print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have the lines form a file; create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XimE1ug_JPu8"
   },
   "outputs": [],
   "source": [
    "#Data class usnign torchtext\n",
    "#from: https://github.com/pytorch/text/blob/master/test/translation.py\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import spacy\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "class Data :\n",
    "    def __init__(self, batch_size=1) :\n",
    "        DE = data.Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        EN = data.Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\" )\n",
    "        self.train, self.val,self.test = datasets.TranslationDataset.splits(\n",
    "                path='data/multi30k/', train='train',\n",
    "                validation='val', exts=('.de', '.en'),\n",
    "                fields=(DE, EN))\n",
    "        DE.build_vocab(self.train.src,  min_freq=3) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        EN.build_vocab(self.train.trg,  max_size=50000) #specials=['<sos>','<eos>','<unk>','<pad>'],\n",
    "        \n",
    "        self.src_lang = DE\n",
    "        self.trg_lang = EN\n",
    "        \n",
    "        #self.train_iter, self.val_iter = data.BucketIterator.splits((self.train,self.val), batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def train_batch(self, batch_size=1) :\n",
    "        self.train_iter, self.val_iter = data.BucketIterator.splits((self.train,self.val), batch_size=batch_size)\n",
    "        for batch in itertools.islice(self.train_iter,0,len(self.train_iter)) :\n",
    "            yield batch.src.view(batch_size,-1), batch.trg.view(batch_size,-1)\n",
    "        \n",
    "    def val_batch(self, batch_size=1) :\n",
    "        self.train_iter, self.val_iter = data.BucketIterator.splits((self.train,self.val), batch_size=batch_size)\n",
    "        for batch in itertools.islice(self.val_iter,0,len(self.val_iter)) :\n",
    "            yield batch.src.view(batch_size,-1), batch.trg.view(batch_size,-1)    \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iv1mj57Z5wSu",
    "outputId": "a9d0a6bc-7d0b-43e1-b33a-e24b6935dd70"
   },
   "outputs": [],
   "source": [
    "multi30k_data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transformer\n",
    "plan is to code the encoder part first.   \n",
    "    \n",
    "We need to have \n",
    "\n",
    "    Scaled Dot Product Attention\n",
    "    Multi head attention\n",
    "    Position wise feed forward\n",
    "    Positional Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#Scaled DotProduct attention\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module) :\n",
    "    \n",
    "    def __init__( self, d_model ) : #d_model is the dimension of the input embeddings\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "        self.scale = np.sqrt( d_model )\n",
    "        \n",
    "    def forward( self, Q, K, V, attn_mask=None ) : #We ignore the msak now\n",
    "        \"\"\"As given in the paper\n",
    "            Q, K, V are matrices\n",
    "            Q    shape => ( batch, n_queries, d_model )\n",
    "            K, V shape => ( batch, N, d_model )\n",
    "        \"\"\"\n",
    "        scores = torch.bmm( Q, K.transpose(1,2) ) / self.scale \n",
    "        #Before softmax we need to apply the mask.\n",
    "        #mask is justt to invalidate certain entries int the scores\n",
    "        if attn_mask is not None :\n",
    "            assert attn_mask.size() == scores.size() \n",
    "            #fill the mask with -inf so that softmax values will be zeros.\n",
    "            scores.data.masked_fill_(attn_mask, -np.inf)\n",
    "            \n",
    "        attn = F.softmax( scores, dim=1 )\n",
    "        out = torch.bmm( attn, V )\n",
    "        \n",
    "        return out, attn \n",
    "\n",
    "#################################################\n",
    "#Multi head Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module) :\n",
    "\n",
    "    def __init__( self, d_model, h, d_k, d_v  ) : #h is the number of heads, d_model == dk in the paper\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.qkv_linear = [ (nn.Linear(d_model, d_k).to(device),nn.Linear(d_model, d_k).to(device),nn.Linear(d_model, d_v).to(device))  \n",
    "                           for _ in range(h) ] #we create h weights for q, k, and v        \n",
    "        self.attn = ScaledDotProductAttention( d_model )        \n",
    "        self.out_linear = nn.Linear( h*d_v, d_model )\n",
    "        \n",
    "    def forward( self, Q, K, V, attn_mask=None ) :\n",
    "        \n",
    "        #just linear transforms\n",
    "        qkv = [ (wq(Q), wk(K), wv(V)) for wq, wk, wv in self.qkv_linear ]\n",
    "        \n",
    "        head_attn =  [ self.attn(q,k,v) for q,k,v in qkv ] #returns a tuple of head and attns\n",
    "        \n",
    "        concat_head = torch.cat( [t[0] for t in head_attn ], -1 ) #t[0] is head\n",
    "        attn = [t[1] for t in head_attn ] #t[1] is the attention\n",
    "        \n",
    "        out = self.out_linear( concat_head )\n",
    "        \n",
    "        return out, attn\n",
    "        \n",
    "        \n",
    "class PositionWiseFFN( nn.Module ) : #position wise ffed forward network\n",
    "    \n",
    "    def __init__( self, d_model, d_hidden ) :\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear( d_model, d_hidden )\n",
    "        #there will be a relu inbetween\n",
    "        self.linear2 = nn.Linear( d_hidden, d_model )\n",
    "\n",
    "    def forward( self, x ) :\n",
    "        out = self.linear2( F.relu( self.linear1(x) ))\n",
    "        return out\n",
    "\n",
    "    \n",
    "#from: https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py\n",
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    #return position_enc\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "\n",
    "        \n",
    "#We find the input embeddings of the incoming index\n",
    "# and add the positional encodings\n",
    "class TransformerEmbedding( nn.Module ) :\n",
    "    def __init__( self, n_vocab, n_position, d_model ) :\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        \n",
    "        self.positional_enc = nn.Embedding(n_position, d_model).from_pretrained( position_encoding_init(n_position, d_model ) )\n",
    "        self.input_emb = nn.Embedding(n_vocab, d_model )\n",
    "        \n",
    "    def forward( self, input_seq, input_positions ) :\n",
    "        #Embedding layer of input + positional encoding is the input \n",
    "        emb_input = self.input_emb( input_seq ) + self.positional_enc( input_positions )\n",
    "        \n",
    "        return emb_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The transformer layer with multihead attention and FFN\n",
    "class TransformerLayer( nn.Module ) :\n",
    "    def __init__( self, d_model, n_head, d_k, d_v, d_hidden ) :\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attn = MultiHeadAttention( d_model, n_head, d_k, d_v )\n",
    "        self.ffn = PositionWiseFFN( d_model, d_hidden )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm( d_model )\n",
    "        self.norm2 = nn.LayerNorm( d_model )\n",
    "        \n",
    "    def forward( self, input ) :\n",
    "        \n",
    "        residual = input \n",
    "        out, attn = self.multi_head_attn( input, input, input )\n",
    "        out += residual #add the residual connection        \n",
    "        ffn_input = self.norm1( out )\n",
    "        \n",
    "        residual = ffn_input\n",
    "        ffn_out = self.ffn( ffn_input )\n",
    "        ffn_out += residual\n",
    "        layer_out = self.norm2( ffn_out )\n",
    "        \n",
    "        return layer_out #, attn \n",
    "    \n",
    "#TransformerNLayers( n_layers, d_model, n_head, d_k, d_v, d_inner_hid )    \n",
    "class TransformerNLayers( nn.Module ) :\n",
    "    def __init__( self, n_layers, d_model, n_head, d_k, d_v, d_hidden ) :\n",
    "        super(TransformerNLayers, self).__init__()\n",
    "        \n",
    "        layer_list = [ TransformerLayer(d_model, n_head, d_k, d_v, d_hidden) for _ in range(n_layers) ]\n",
    "        self.layers = nn.Sequential( *layer_list )\n",
    "    \n",
    "    def forward( self, input ) :\n",
    "        return self.layers( input )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Encoder \n",
    "\n",
    "class TransformerEncoder( nn.Module ) :\n",
    "    \n",
    "    def __init__( self, n_src_vocab, n_max_seq, n_layers=6, d_model=512, n_head=8, d_k=64, d_v=64,\n",
    "                 d_word_vec=512, d_inner_hid=1024, dropout=0.1) :\n",
    "        \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        n_position = n_max_seq + 1\n",
    "        self.n_max_seq = n_max_seq\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        #for processing the input\n",
    "        self.input_emb = TransformerEmbedding( n_src_vocab, n_position, d_model )\n",
    "        \n",
    "        #N layers of transformer\n",
    "        self.layers = TransformerNLayers( n_layers, d_model, n_head, d_k, d_v, d_inner_hid )\n",
    "        \n",
    "    def forward( self, input_seq, input_positions) :\n",
    "        \n",
    "        #Embedding layer of input + positional encoding is the input\n",
    "        emb_input = self.input_emb( input_seq, input_positions )\n",
    "        \n",
    "        # pipe it through all the N layers\n",
    "        #enc_output, enc_slf_attn = self.layers( enc_input )\n",
    "        enc_output = self.layers( emb_input )\n",
    "        \n",
    "        return enc_output #, enc_slf_attns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "batch = multi30k_data.val_batch(batch_size=1)\n",
    "x,y = next(batch)\n",
    "print (x.shape, y.shape)\n",
    "#e = TransformerEncoder( data.src.n_words, 10 ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "n_src_vocab = len(multi30k_data.src_lang.vocab)\n",
    "e = TransformerEncoder(n_src_vocab, n_max_seq=50 ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print (x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = e.forward( x, torch.tensor([0,1,2,3,4], dtype=torch.long).view(1,-1).to(device) )\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "input_emb = TransformerEmbedding( n_src_vocab, 50, d_model ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = input_emb(x, torch.tensor([[0],[1],[2],[3],[4]], dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 256])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2],\n",
       "        [  5],\n",
       "        [ 12],\n",
       "        [  0],\n",
       "        [  3]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "out = embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4]), torch.Size([1, 4, 3]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ScaledDotProductAttention(10)\n",
    "a = MultiHeadAttention(h=8, d_model=10, d_k=12, d_v=15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.ones( 2, 1, 10)\n",
    "K = torch.ones(2,33,10)\n",
    "V = torch.ones(2,33,10)\n",
    "p,q = a.forward(Q,K,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape #, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,4)\n",
    "n = torch.cat( [x, x] , -1 )\n",
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pe( n_positions, d_model ) :\n",
    "    j = np.arange(0,d_model) * (2.0/d_model)\n",
    "    q = np.array( [pos/np.power( 10000, j ) for pos in range(n_positions)] ).reshape( -1, d_model )\n",
    "    #We will apply sin and cos to q\n",
    "    position_enc = q\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return position_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = pe( 9, 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return position_enc\n",
    "    #return torch.from_numpy(position_enc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = position_encoding_init( 9, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.Tensor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzl4TVrG9kqS"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, src_vocab_size, hidden_size, num_layers=1 ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_vocab_size, self.enbedding_vector_size )\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #seq_length, batch, enbbding\n",
    "        #print (output.shape)\n",
    "        #print (hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F8gYijaAgbN"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, dest_vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dest_vocab_size, self.enbedding_vector_size )\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, dest_vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1)\n",
    "        #output = F.relu(output)\n",
    "        output = embedded.view( input.shape[0], 1, -1 ) #input shape[0] is 1 as wqe feed one input at a time.\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear( output.squeeze() )\n",
    "        #print (output.shape)\n",
    "        output = F.log_softmax( output, dim=0 )\n",
    "        return output.view(1,-1), hidden #output of shape N,C; here N=1\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E_DKBJ5AjkK"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class Attn(nn.Module) :\n",
    "    def __init__(self, hidden_size, max_length) :\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.linear = nn.Linear(self.hidden_size, self.max_length)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs) :\n",
    "        \n",
    "        attn_scores = self.linear(hidden)\n",
    "        #print(\"attn_scores\", attn_scores.shape )\n",
    "                \n",
    "        attn_weights = F.softmax(attn_scores, dim=2)\n",
    "        \n",
    "        #print(\"attn_weights\", attn_weights.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        \n",
    "        attn_applied = torch.matmul(attn_weights.squeeze(),encoder_outputs)\n",
    "        #print (\"attn_applied \", attn_applied.shape)\n",
    "        \n",
    "        return attn_applied, attn_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #embedding vector size is fixed as hidden size\n",
    "        self.enbedding_vector_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.enbedding_vector_size)\n",
    "        \n",
    "        self.attn = Attn(self.hidden_size, self.max_length)\n",
    "        #self.attn = nn.Linear(self.hidden_size, self.max_length)        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"input is an index of the word. We create a word vector out of it\"\"\"\n",
    "        embedded = self.embedding(input) \n",
    "        #print(\"embedded\", embedded.shape )\n",
    "                \n",
    "        \"\"\" gru hidden has shape (num_layers * num_dir, batch, hidden_size)\n",
    "            Here first two dim are 1\n",
    "        \"\"\"\n",
    "        output, hidden = self.gru(embedded.view(1,1,-1), hidden)\n",
    "        #print (\"hidden \", hidden.shape)\n",
    "        \n",
    "        #linear W.h \n",
    "        #out (max, )\n",
    "        attn_context, attn_weights = self.attn( hidden, encoder_outputs)\n",
    "        #print (\"attn_context \", attn_context.shape)\n",
    "        \n",
    "        \n",
    "        output = torch.cat((hidden.view(1,-1), attn_context.view(1,-1)), 1)\n",
    "        #print (\"output \", output.shape) \n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        #print (\"output \", output.shape)        \n",
    "        output = F.relu(output) #h tilde\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        #output = F.log_softmax(self.out(output), dim=1)\n",
    "        output = self.out(output)\n",
    "        #print (\"output \", output.shape)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SJIEKfqAoVR"
   },
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate( encoder, decoder, data, input_sentence ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = data.line_to_tensor( data.src, input_sentence ).to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    next_word = data.index_to_tensor( data.dest.SOS_token ).to(device)\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        if next_word.item() == data.dest.EOS_token :\n",
    "            break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "        \n",
    "    return \" \".join([ data.dest.index2word[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_list = []\n",
    "def translate2( encoder, decoder, x ) :\n",
    "    debug_list = [] #XXX\n",
    "    x = x.to(device)\n",
    "    h = encoder.initHidden().to(device)\n",
    "    out, h = encoder(x, h)\n",
    "    g = h\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for i in range(out.shape[0]) :\n",
    "        encoder_outputs[i] = out[i][0]\n",
    "        \n",
    "    #first input is SOS\n",
    "    #next_word = in_data.index_to_tensor( in_data.trg_lang.SOS_token ).to(device)\n",
    "    next_word = x[0]\n",
    "    predicted_target = []\n",
    "    for _ in range(25) :        \n",
    "        scores, g, attn_w = decoder( next_word, g, encoder_outputs )\n",
    "        #debug_list.append(attn_w)\n",
    "        #if next_word.item() == in_data.trg_lang.EOS_token :\n",
    "        #    break\n",
    "        predicted_target.append( next_word.item() )\n",
    "        if next_word.item() == 3 : #in_data.trg_lang.EOS_token :\n",
    "            break\n",
    "        #now we make the next_word from current_word\n",
    "        v, next_word = scores.topk(1) #return value and index\n",
    "        #new_word = data.index_to_tensor( next_word )\n",
    "        #next_word = torch.multinomial( torch.exp(scores), 1 )[0]\n",
    "        #next_word = torch.multinomial( scores, 1 )[0]\n",
    "        \n",
    "    return predicted_target    \n",
    "    #return \" \".join([ in_data.trg_lang.itos[i] for i in predicted_target ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAefxwrWA8mL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=5000 ) :\n",
    "    start = time()\n",
    "    #batch = multi30k_data.batch(n_data=n_data, random=True)\n",
    "    train_iter = multi30k_data.train_batch()\n",
    "    \n",
    "    loss_db = []\n",
    "    for x, y in train_iter :\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        h = encoder.initHidden().to(device)\n",
    "        h.detach_()\n",
    "\n",
    "        out, h = encoder(x, h)\n",
    "        g = h\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        for i in range(out.shape[0]) :\n",
    "            encoder_outputs[i] = out[i][0]\n",
    "    \n",
    "        for i in range(len(y) - 1) :\n",
    "        #for i in range(1) :\n",
    "            scores, g, attn_w = decoder( y[i], g, encoder_outputs )\n",
    "            #print(scores.shape)\n",
    "            #print(next_word.shape)\n",
    "            \n",
    "            loss += criterion(scores, y[i+1] )\n",
    "            #next_word = sample_from_scores( scores )  \n",
    "            #next_word = sample_from_softmax( scores )\n",
    "\n",
    "            #next_word = data.index_to_tensor( next_word )\n",
    "\n",
    "        loss.backward()\n",
    "        loss_db.append( float(loss) )\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        if n_data < 0 :\n",
    "            break\n",
    "        else :\n",
    "            n_data -= 1\n",
    "        \n",
    "    end = time()\n",
    "    print (end-start)\n",
    "    return loss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Jl135e7-Ar-U",
    "outputId": "282ffa66-6fb7-4734-99d5-137d2273e0ff"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(data.src.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, data.dest.n_words).to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#criterion = nn.NLLLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "y1QUXIJOA1XO",
    "outputId": "e514b108-6037-4473-c246-f0ec69bd09ac"
   },
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "for _ in range(100) :\n",
    "    l = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_data=1000 )\n",
    "    avg_loss.append( np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "Vwrih715BCtO",
    "outputId": "28f1e505-027a-4c0d-9bf5-ece32a3ec9dd"
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5mf8Uw7-BUv1",
    "outputId": "ecbd0add-e65a-4659-c8e9-071b54dfd0cb"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "#train_iter = multi30k_data.train_batch()\n",
    "inp, out = next(train_iter)\n",
    "target_output   = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in out ]) \n",
    "tout = translate2( encoder, decoder, inp )\n",
    "\n",
    "output_sentence = \" \".join([ multi30k_data.trg_lang.vocab.itos[i] for i in tout ])\n",
    "print(target_output)\n",
    "#print(multi30k_data__.trg_lines[i])\n",
    "print(output_sentence)\n",
    "print( sentence_bleu( [target_output.split(' ')], output_sentence.split(' ') , smoothing_function=SmoothingFunction().method1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npzWULkpCHWA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_nmt_with_attn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
